{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 6: optimization and learning**\n",
        "**Gustav Grevsten**"
      ],
      "metadata": {
        "id": "0cxDwmWNlmFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Abstract**\n",
        "\n",
        "The purpose of this lab is to implement and test the gradient descent method algorithm for finding stationary points in multivarable functions. We implement it using the golden-section search method to calculate the step length. In the end, we conclude that the algorithms implemented yield results that were expected."
      ],
      "metadata": {
        "id": "Pq7oUC3WIUbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set up environment**"
      ],
      "metadata": {
        "id": "5F3Jeg16l2tN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xXxu_byildMJ"
      },
      "outputs": [],
      "source": [
        "# Load neccessary modules.\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "Gradient descent is an optimization technique that involves finding the minimum or maximum value of a multivariable function $f: \\mathbb{R}^n â†’ \\mathbb{R}, n \\in \\mathbb{N}$ by iteratively adjusting the parameters in the direction of the steepest descent or ascent of the function. This method is useful for finding stationary points, which are locations where the gradient of the function is the zero vector. In essence, the gradient descent algorithm works by following or going against the direction of the gradient until a minimum or maximum point is reached, and can be adapted to handle functions of varying complexity and dimensionality. Despite its simplicity, gradient descent is a powerful tool for solving optimization problems and has become a fundamental technique in modern data science and engineering."
      ],
      "metadata": {
        "id": "79Yb3kIELJOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Method**\n",
        "\n",
        "In this lab we will employ the gradient descent method, or steepest descent method, in order to find stationary points for multivariable functions. \n",
        "\n",
        "Starting from an initial guess vector $\\overline{x}_0$, we update our guess by taking a step in the direction of the negative gradient of the function $f(\\overline{x})$ at $x_n$, $n \\in \\mathbb{N}$:\n",
        "\n",
        "$$x_{n+1} = x_n - \\alpha \\nabla f(x_n)$$\n",
        "\n",
        "where $\\alpha$ is a scalar called the step length, and $\\nabla f(x_n)$ is the gradient of the function evaluated at $x_n$. This process is repeated iteratively until a convergence criterion is met, which in this case is a sufficiently small norm $||\\nabla f(x_n)||$.\n",
        "\n",
        "In this case, $\\alpha$ is determined using the golden-section search method to minimize the function within the interval $\\left[x_n, x_n - \\frac{\\nabla f(x_n)}{||\\nabla f(x_n)||}\\right]$.\n"
      ],
      "metadata": {
        "id": "WKU7zMwBWHm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def line_search(f, a, b, thresh = 10**-2):\n",
        "    \"\"\"Golden-section search\"\"\"\n",
        "    gr = (np.sqrt(5) + 1) / 2\n",
        "    c = np.subtract(b, np.multiply(np.subtract(b, a), 1/gr))\n",
        "    d = np.add(a, np.multiply(np.subtract(b, a), 1/gr))\n",
        "    while np.linalg.norm(np.subtract(b, a)) > thresh:\n",
        "        if f(c) < f(d):\n",
        "            b = d\n",
        "        else:\n",
        "            a = c\n",
        "\n",
        "        c = np.subtract(b, np.multiply(np.subtract(b, a), 1/gr))\n",
        "        d = np.add(a, np.multiply(np.subtract(b, a), 1/gr))\n",
        "\n",
        "    return np.multiply(np.add(b, a), 1/2)\n",
        "\n",
        "def grad(F, x):\n",
        "  dX = 10**-10\n",
        "  dF = []\n",
        "  for i in range(len(x)):\n",
        "    X = x.copy()\n",
        "    X[i] += dX\n",
        "    dF.append(np.subtract(f(X), f(x)))\n",
        "  return np.multiply(dF, 1/dX)\n",
        "\n",
        "def grad_dec(f, x0, tresh = 10**-5):\n",
        "  dF = grad(f, x0)\n",
        "  x = x0\n",
        "  while np.linalg.norm(dF) > tresh:\n",
        "    dF = grad(f, x)\n",
        "    dF_unit = np.multiply(dF, 1/(np.linalg.norm(dF)))\n",
        "    alpha = np.linalg.norm(np.subtract(x, line_search(f, x, np.subtract(x, dF_unit))))\n",
        "    x = np.subtract(x, np.multiply(dF, alpha))\n",
        "  return x"
      ],
      "metadata": {
        "id": "Ai7uRF6uluMO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**\n",
        "\n",
        "Here, we test the algorithm presented in the methods section. We use objective function $f(x,y) = sin(x) + cos(y)$ with the gradient $\\nabla f(x,y) = [cos(x), -sin(y)]^T$. This function has the stationary points $(\\frac{\\pi}{2} + n\\pi, n\\pi)$, $n \\in \\mathbb{Z}$. We test the algorithm using the arbitrary starting guesses $(0,0), (1,1)$ and $(-1,-1)$"
      ],
      "metadata": {
        "id": "dfFuDYEzhFv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(v):\n",
        "  x, y = v[0], v[1]\n",
        "  return np.sin(x) + np.cos(y)\n",
        "\n",
        "print(\"Stationary point found for initial guess (0,0): \" + str(grad_dec(f, [0,0])))\n",
        "print(\"Stationary point found for initial guess (1,1): \" + str(grad_dec(f, [1,1])))\n",
        "print(\"Stationary point found for initial guess (-1,-1): \" + str(grad_dec(f, [-1,-1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya0TMzicjP09",
        "outputId": "da7e3275-d20b-4181-d1dc-11cfb288fa37"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stationary point found for initial guess (0,0): [-1.57078545  0.        ]\n",
            "Stationary point found for initial guess (1,1): [-1.57078467  3.14159176]\n",
            "Stationary point found for initial guess (-1,-1): [-1.57079529 -3.14158073]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the algorithm converged to three separate stationary points for each of the starting points and all solutions have accurate values."
      ],
      "metadata": {
        "id": "BPTiyy_Ax18A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discussion**\n",
        "\n",
        "As expected, the results confirm the effectiveness of this optimization technique for finding stationary points in functions. It should be noted that the step length $\\alpha$ can be calculated using other methods, such as the conjugate gradient method, and a different interval could have been chosen for the golden-section search. The choice of both of these can have an impact on the convergence of the results.\n",
        "\n",
        "Overall, the results provide further evidence of the usefulness and versatility of the gradient descent method for optimization problems."
      ],
      "metadata": {
        "id": "YuSqaMcwx-DD"
      }
    }
  ]
}