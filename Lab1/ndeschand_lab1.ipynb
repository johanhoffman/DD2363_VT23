{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT23/blob/main/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"6RgtXlfYO_i7"},"source":["# **Lab 1: Matrix Factorization**\n","**Nolwenn Deschand**"]},{"cell_type":"markdown","metadata":{"id":"9x_J5FVuPzbm"},"source":["# **Abstract**"]},{"cell_type":"markdown","metadata":{"id":"6UFTSzW7P8kL"},"source":["\n","The first lab is about Matrix Factorization. \n","Matrix Factorization is a useful tool for solving systems of linear equations such as $Ax = b$. The solution, $x = A^{-1}b$  requires to be able to compute $A^{-1}$. For some matrices, it can be a complex and costly operation. The goal of Matrix Factorization is to factorize the matrix A into a product of matrices that are easily invertible, such as orthogonal or upper triangular matrices.\n","\n","---\n","\n","Most of the algorithms are implemented from the pseudo-code present in the Chapter 5 of the book *Methods in Computational Science*, from Johan Hoffman.\n"]},{"cell_type":"markdown","metadata":{"id":"OkT8J7uOWpT3"},"source":["#**About the code**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pdll1Xc9WP0e","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1674909199548,"user_tz":-60,"elapsed":279,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}},"outputId":"5354515e-9284-4e4b-b1f5-5e3275c01d82"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'KTH Royal Institute of Technology, Stockholm, Sweden.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":61}],"source":["\"\"\"This program is a lab report using the provided template\"\"\"\n","\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n","\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n","\n","# written by Nolwenn Deschand (ddeschand@kth.se)\n","# Template by Johan Hoffman\n"]},{"cell_type":"markdown","metadata":{"id":"28xLGz8JX3Hh"},"source":["# **Set up environment**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xw7VlErAX7NS"},"outputs":[],"source":["# Load neccessary modules.\n","from google.colab import files\n","\n","import time\n","import numpy as np\n"]},{"cell_type":"markdown","metadata":{"id":"gnO3lhAigLev"},"source":["# **Introduction**"]},{"cell_type":"markdown","metadata":{"id":"l5zMzgPlRAF6"},"source":["In this lab, we will work on implementation of Matrix Factorization methods in order to solve systems of linear equations as $Ax = b$, where A is a matrix, x the unknown solution and b a vector. This type of systems are common for the study of differential and integral equations. We can write the solution x as $x = A^{-1}b$. However, inverting the matrix A cas be very costly and difficult, for instance in the cases of large matrices. \n","\n","In this lab, we will implement and test several functions related to matrix factorization : a sparse vector-matrix product, a QR_factorization (Gram-Schmidt QR Factorization), a direct solver and a QR eigenvalue algorithm.\n","\n","For each function implemented, we will explain the method used, implement it, test it and then discuss the results obtained. \n"]},{"cell_type":"markdown","metadata":{"id":"jOQvukXZq5U5"},"source":["# **Method**"]},{"cell_type":"markdown","metadata":{"id":"zF4iBj5VURZx"},"source":["\n","**Implementation of the sparse matrix-vector product**\n","\n","There is a type of matrix called sparse matrices. These matrices are mostly composed of zero components, that is the number of nonzero components is O(n).\n","\n","For these matrices, storing all components is costly. Instead, we can represent it with the CRS data structure (compressed row storage), composed of 3 arrays:\n","\n","\n","*   val, that contains the nonzero components of the matrix in row order\n","*   col_idx, that contains the index of the column of each nonzero component of the matrix\n","* row_ptr, that contains the indices in the other two arrays that correspond to the start of each row and which ends with the number of nonzero components plus 1\n","\n","Here, we will try to implement the product $b = Ax$ with A a sparse matrix and x a vector.\n","\n","Therefore we  will take as input: a vector x, a sparse (real, quadratic) matrix A represented in CRS arrays: val, col_idx, row_ptr\n","\n","The expected output is the matrix-vector product b=Ax\n","\n","Pseudo code of the algorithm (from book Chapter 5): \n","\n","\n","```\n","ALGORITHM 5.9. b = sparse_matrix_vector_product(A, x).\n","Input: a sparse m x n matrix A and an n vector x.\n","Output: the matrix-vector product b = Ax\n","1: for i=0:n-1 do\n","2:     b[i]=0\n","3:     for j=A.row_ptr[i]:A.row_ptr[i+1]-1 do\n","4:         b[i]= b[i] + A.val[j]*x[A.col_idx[j]]\n","5:     end for\n","6: endfor\n","7: return b\n","\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dH6KZlIUWGBj"},"outputs":[],"source":["def sparse_mv_product(x, val, col_idx, row_ptr):\n","    n = x.shape[0]\n","\n","    #Algorithm \n","    b = np.zeros(n)\n","    for i in range(n):\n","        b[i] = 0\n","        for j in range (row_ptr[i], row_ptr[i+1]):\n","            b[i] = b[i] + val[j-1] * x[col_idx[j-1]-1]\n","    return b"]},{"cell_type":"markdown","metadata":{"id":"Y2QWfU6tWGBk"},"source":["**Implementation of the Gram-Schmidt QR factorization**\n","\n","In order to solve the linear eqution $Ax = b$, we need to compute the inverse of A. As computing the inverse is complicated, we can try to factorize A into easily invertible matrices. With the QR factorization, we will express $A = QR$ with Q an orthogonal matrix and R an upper triangular matrix.\n","\n","The algorithm will take as input a real, quadratic, invertible matrix A.\n","The expected output is an orthogonal matrix Q and an upper triangular matrix R, such that $A=QR$.\n","\n","Pseudo code of the algorithm (from Chapter 5):\n","```\n","ALGORITHM 5.3. (Q, R) = modified_gram_schmidt_iteration(A). \n","Input: a full rank n x n matrix A.\n","Output: an orthogonal n x n matrix Q and an upper triangular n x n matrix R.\n","1: for j=0:n-1do\n","2:     v[:] = A[:,j]\n","3:     for i=0:j-1 do\n","4:         R[i,j] = scalar_product(Q[:,i], v[:])\n","5:         v[:] = v[:] - R[i,j]*Q[:,i]\n","6:     end for\n","7:     R[j,j] = norm(v)\n","8:     Q[:,j] = v[:]/R[j,j]\n","9: endfor\n","10:return Q, R\n","\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"by5FsRUcWGBk"},"outputs":[],"source":["def QR_factorization(A):\n","\n","    n = A.shape[0]\n","\n","    R = np.zeros((n,n))\n","    Q = np.zeros((n,n))\n","\n","    v = np.empty(n, dtype=object)\n","\n","    # Algorithm\n","    for j in range (n):\n","        v = A[:,j]\n","        for i in range (j): \n","            R[i, j] = np.dot(Q[:,i], v)\n","            sum = R[i,j]*Q[:,i]\n","            v = v - sum\n","        R[j,j] = np.linalg.norm(v)\n","        Q[:,j] = v[:]/R[j,j]\n","\n","    return Q,R"]},{"cell_type":"markdown","metadata":{"id":"_ywxp87PWGBl"},"source":["**Implementation of a direct solver Ax=b**\n","\n","Using the previously implemented QR Factorization, we can know implement a direct solver for $Ax = b$. As, we have $A = QR$, we can rewrite the equation as follow: \n","\n","$Ax = b ⇔ QRx = b ⇔ Rx = Q^{-1}b ⇔ x = R^{-1}Q^{-1}b$\n","\n","The solver will take as input a real quadratic matrix A, and a vector b.\n","The expected output is a vector $x = A^{-1}b$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Coln9h16WGBl"},"outputs":[],"source":["def solver(A,b):\n","    Q,R = QR_factorization(A)\n","    x = np.dot(np.dot(np.linalg.inv(R), np.linalg.inv(Q)),b)\n","    return x"]},{"cell_type":"markdown","source":["**Bonus assignement : Implementation of a the QR eigenvalue algorithm**\n","\n","There are algorithms for computing the eigenvalue decomposition of matrices, as the QR eigenvalue algorithm. \n","\n","For a square matrix A, the Schur factorization is constructed from successive QR factorizations\n","$Q^{(k)}R^{(k)} = A^{(k−1)}$\n","\n","After k iterations we have the approximate Schur factorization :\n","$A = U^{(k)}A^{(k)}U^{(k)∗}$\n","where under suitable conditions $A^{(k)}$ will converge to an upper triangular matrix which has the eigenvalues of A on the diagonal.\n","\n","To simplify the analysis of the convergence properties we can restrict our attention to matrices that are real and symmetric, for which all eigenvalues are real and the corresponding eigenvectors form an orthonormal basis for $R^{n}$\n","\n","As input, we wil take a real symmetric matrix A.\n","\n","The output are real eigenvalues $\\lambda_{i}$ and real eigenvectors $v_{i}$ of A.\n","\n","Pseudo code of thz algorithm (from Chapter 6):\n","```\n","ALGORITHM 6.1. (A, U) = qr_algorithm(A).\n","Input: a general n x n matrix A.\n","Output: approximate Schur factorization n x n matrices A and U.\n","1: U=I \n","2: while stopping_criterion == false do\n","3:     (Q, R) = qr_factorization(A)\n","4:     A = matrix_matrix_product(R, Q)\n","5:     U = matrix_matrix_product(U, Q)\n","6: end while\n","7: return A, U\n","```\n","\n","There are several possible choices for the stopping criterion. Here, I chose to limit the non diagonal residuals: by computing the sum of the elements in A minus the sum of the elements of the diagonal, we get the sum of the non diagonal elements. This value should be as close to zero as possible. We can adjust the threshold to get more accurate results. "],"metadata":{"id":"F6n6U7CNDI0h"}},{"cell_type":"code","source":["def qr_algorithm(A):\n","\n","  n = A.shape[0]\n","  U = np.identity(n)\n","\n","  non_diag_residuals = A.sum() - A.trace()\n","\n","  while non_diag_residuals > 0.000001 :\n","    (Q, R) = QR_factorization(A)\n","    A = np.dot(R, Q)\n","    U = np.dot(U, Q)\n","    non_diag_residuals = A.sum() - A.trace()\n","\n","  return A, U"],"metadata":{"id":"P_-QysAKDuj_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SsQLT38gVbn_"},"source":["# **Results**"]},{"cell_type":"markdown","metadata":{"id":"RLwlnOzuV-Cd"},"source":["In this section, all the tests of the previously implemented functions will be realized. They will be explained, the expected results will be presented and compared to the obtained results.\n","\n","---\n","\n","#**Sparse matrix-vector product**\n","\n","The sparse matrix-vector product $Ax = b$ has been implemented, we will know test it. In order to test the implementation, we will compare the result of the implemented function with a dense matrix-vector product, using numpy library. If the function is correctly implemented, the results should be equal.\n","\n","A is a sparse matrix, and val, col_idx and row_ptr are its CRS representation. \n","\n"]},{"cell_type":"code","source":["# Matrix A \n","A = [[3, 2, 0, 2, 0, 0], [0, 2, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 3, 2, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 2, 3]]\n","\n","# CRS Representation \n","val = [3, 2, 2, 2, 1, 1, 3, 2, 1, 2, 3]\n","col_idx = [1, 2, 4, 2, 3, 3, 3, 4, 5, 5, 6]\n","row_ptr = [1, 4, 6, 7, 9, 10, 12]\n","\n","# Vector x\n","x = np.array([2, 0, 1, 4, 5, 2])\n","\n","b = sparse_mv_product(x, val, col_idx, row_ptr)\n","print(\"sparse matrix vector product : \", b)\n","\n","# Test : Dense product \n","print (\"dense matrix vector product : \",np.dot(A,x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KoBs6DtHwG3","executionInfo":{"status":"ok","timestamp":1674909199808,"user_tz":-60,"elapsed":27,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}},"outputId":"dc3e2fc1-4e3a-482c-95a8-38e3744c19b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sparse matrix vector product :  [14.  1.  1. 11.  5. 16.]\n","dense matrix vector product :  [14  1  1 11  5 16]\n"]}]},{"cell_type":"markdown","source":["\n","We obtain the same results for the two implementations of the matrix vector product, the sparse matrix-vector product implementation seems to be correct on this example. "],"metadata":{"id":"t9lVhGMKvYRk"}},{"cell_type":"markdown","source":["#**Gram-Schmidt QR factorization**\n","\n","To test the QR factorization, we will verify that R is an upper triangular matrix and that Q is orthogonal. To measure the size of a matrix $A ∈ R^{mxn}$, we can use the Frobenius norm of the matrices, defined by:\n","$||A||_{F} = trace(A^{T}A) = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}|a_{ij}|^{2}}$. We can use the numpy implementation of the Frobenius norm.\n","\n","We will compute the norms $||Q^{T}Q-I||_{F}$ and $||QR-A||_{F}$.\n","As Q should be orthogonal, we should have $Q^{T} = Q^{-1}$ therefore $Q^{T}Q - I$ should result in the null matrix, with a frobenius norm equal to 0. \n","\n","If the factorization is correct, we should have $A = QR$, therefore the norm $||QR-A||_{F}$ should also be 0."],"metadata":{"id":"PL9_7PR0Hvcl"}},{"cell_type":"code","source":["# Test\n","A = np.array([[3, 2, 1, 2], [6, 2, 1, 2], [3, 5, 1, 4], [5, 5, 3, 2]])\n","\n","Q,R = QR_factorization(A)\n","print(\"R = \",R)\n","\n","# Frobenius norm \n","n = A.shape[0]\n","\n","F1 = np.linalg.norm(np.dot(np.transpose(Q), Q)-np.identity(n),'fro')\n","\n","F2 = np.linalg.norm(np.dot(Q,R)-A,'fro')\n","print(\"F1 = \", F1)\n","print(\"F2 = \", F2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CNjUPRRTHxE5","executionInfo":{"status":"ok","timestamp":1674909199810,"user_tz":-60,"elapsed":26,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}},"outputId":"f3a12597-bf9d-498e-80a2-b1abba671762"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R =  [[ 8.88819442  6.52550983  3.03773733  4.5003516 ]\n"," [ 0.          3.92654066  1.06384106  2.19860487]\n"," [ 0.          0.          1.2807787  -1.56924238]\n"," [ 0.          0.          0.          0.67115606]]\n","F1 =  4.938266190042341e-16\n","F2 =  4.440892098500626e-16\n"]}]},{"cell_type":"markdown","source":["The obtained R is as expected an upper triangular matrix. \n","\n","The two computed frobenius norms $||Q^{T}Q-I||_{F}$ and $||QR-A||_{F}$ are very close to 0, we can consider that the implementation is working as expected for this test case. "],"metadata":{"id":"NxAoPt8Xvvzf"}},{"cell_type":"markdown","source":["#**Direct solver Ax=b**\n","\n","To test the direct solver, we can first compute the norm of the residual $||Ax-b||$.\n","We can also try to manufacture a solution (choose a matrix A and a vector y and compute $b = Ay$ and compare y with the result of the solver. \n","\n"],"metadata":{"id":"fMgo_4JDHyZ8"}},{"cell_type":"code","source":["# TEST 1: residual of ||𝐴𝑥−𝑏||\n","\n","A = np.array([[3, 2, 1, 2], [6, 2, 1, 2], [3, 5, 1, 4], [5, 5, 3, 2]])\n","b = np.array([2,3,1,2])\n","x = solver(A,b)\n","\n","residual1 = np.linalg.norm(np.dot(A,x)-b)\n","print(\"residual of ||𝐴𝑥−𝑏|| : \", residual1)\n","\n","# TEST 2: residual of ||x-y|| where y is a manufactured solution with b=Ay\n","\n","A = np.array([[5, 9, 1, 2], [1, 3, 3, 2], [4, 4, 2, 1], [6, 2, 1, 1]])\n","y = np.array([3,1,1,2])\n","\n","b = np.dot(A,y)\n","\n","x = solver(A,b)\n","residual2 = np.linalg.norm(np.dot(A,x)-b)\n","print(\"residual of ||x-y|| : \", residual2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N35HGnVkHzNf","executionInfo":{"status":"ok","timestamp":1674909199811,"user_tz":-60,"elapsed":24,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}},"outputId":"188077fe-2356-4d0e-c450-54a488edd4a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["residual of ||𝐴𝑥−𝑏|| :  1.0877919644084146e-15\n","residual of ||x-y|| :  8.702335715267317e-15\n"]}]},{"cell_type":"markdown","source":["For both test cases, the resulted norm is closed to 0, as expected."],"metadata":{"id":"9eCaovjp9UHm"}},{"cell_type":"markdown","source":["#**Bonus assignement : Implementation of a the QR eigenvalue algorithm**\n","\n","To test the implementation of the QR eigen value algorithm, we will compute for each pair of eigenvalue and eigenvector:\n","\n","1. $det(A - \\lambda_{i}I)$,\n","2. $||Av_{i} - \\lambda_{i}v_{i}||$\n","\n","With $\\lambda_{i}$ the eigenvalue and $v_{i}$ the associated eigenvector.\n","\n","The implemented function should return a diagonal matrix A with the eigenvalues on the diagonal and a matrix U with the eigenvevtors on the columns.\n","\n","If the function is correctly implemented the determinant $det(A - \\lambda_{i}I)$ and the norm $||Av_{i} - \\lambda_{i}v_{i}||$ should be 0, as the definition of eigenvalues and eigenvectors is $Av_{i} = \\lambda_{i}v_{i}$.\n"],"metadata":{"id":"-jOycc4DO5P5"}},{"cell_type":"code","source":["# A, a real symmetric matrix\n","A = np.array([[2, 4, 7, 3], [4, 3, 2, 5], [7, 2, 1, 8], [3, 5, 8, 6]])\n","\n","(Adiag,U) = qr_algorithm(A)\n","\n","n = A.shape[0]\n","\n","for i in range(n):\n","  print (\"Eigenvalue \",i+1,\": \")\n","\n","  print(\"𝑑𝑒𝑡(𝐴−𝜆𝑖𝐼) : \",np.linalg.det(A-np.dot(Adiag[i,i],np.identity(n))))\n","  print(\"||𝐴𝑣𝑖−𝜆𝑖𝑣𝑖|| : \", np.linalg.norm(np.dot(A,U[:,i])-Adiag[i,i]*U[:,i]))\n","  print()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KhmUygUMKlGi","executionInfo":{"status":"ok","timestamp":1674909226680,"user_tz":-60,"elapsed":307,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}},"outputId":"65e5ff28-2bbc-474d-cdd4-884fca3e1e55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Eigenvalue  1 : \n","𝑑𝑒𝑡(𝐴−𝜆𝑖𝐼) :  5.160851235385878e-12\n","||𝐴𝑣𝑖−𝜆𝑖𝑣𝑖|| :  6.4047456679787536e-15\n","\n","Eigenvalue  2 : \n","𝑑𝑒𝑡(𝐴−𝜆𝑖𝐼) :  0.0\n","||𝐴𝑣𝑖−𝜆𝑖𝑣𝑖|| :  2.7012892057857038e-15\n","\n","Eigenvalue  3 : \n","𝑑𝑒𝑡(𝐴−𝜆𝑖𝐼) :  3.4833662193366036e-11\n","||𝐴𝑣𝑖−𝜆𝑖𝑣𝑖|| :  4.892066786674748e-07\n","\n","Eigenvalue  4 : \n","𝑑𝑒𝑡(𝐴−𝜆𝑖𝐼) :  3.465503324834715e-11\n","||𝐴𝑣𝑖−𝜆𝑖𝑣𝑖|| :  4.892066785901544e-07\n","\n"]}]},{"cell_type":"markdown","source":["All the values here are very close to zero, which is the expected behaviour for eigenvalues and eigenvectors. We could probably improve the precision by lowering the threshold in the QR eigenvalue algorithm, at the cost of increasing computations."],"metadata":{"id":"p7SWuLHMKmEA"}},{"cell_type":"markdown","metadata":{"id":"_4GLBv0zWr7m"},"source":["# **Discussion**"]},{"cell_type":"markdown","metadata":{"id":"6bcsDSoRXHZe"},"source":["Finally, we have implemented four methods for matrix products and factorization. For the simple test cases realized, the implementations seem to be working as all expected results were obtained."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"},"vscode":{"interpreter":{"hash":"20734bf63f68e910b70bcd66688426a9a04b21c2f9ae6e19ab556dd9ed69c768"}}},"nbformat":4,"nbformat_minor":0}