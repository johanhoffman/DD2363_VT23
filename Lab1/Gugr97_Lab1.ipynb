{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 1: Matrix factorization**\n",
        "**Gustav Grevsten**"
      ],
      "metadata": {
        "id": "LiIDSp16dSba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Abstract**"
      ],
      "metadata": {
        "id": "iBgVYKZ_dc_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this lab is to implement and test algorithms for calculating a sparse matrix-vector product, QR factorization of matrices that are real, quadratic and invertible, and for directly solving for vectors $x=A^{-1}b$ for a given $b$ and $A$. In the end, we conclude that the algorithms implemented yield results that were expected.\n",
        "\n"
      ],
      "metadata": {
        "id": "so4Wm-9-diWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set up environment**"
      ],
      "metadata": {
        "id": "wu-p4I2PfT6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load neccessary modules.\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aqBzg8jXfaU9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "Linear operators play an important role in many scientific computations, as they provide a powerful mathematical tool for modeling real-world phenomena. To this end, it is often useful to be able to effectivize computations involving them and to use various techniques for comptuting difficult problems.\n",
        "\n",
        "The compressed row storage (CRS) format is a technique for storing sparse matrices, which are matrices with mostly zeroes in their entries. This is useful in many applications, such as numerical simulations, optimization, and data analysis, where large sparse matrices arise naturally. The sparse structure of the matrices means that the CRS format can be used for computing matrix-vector products with reduced computational cost and memory requirements.\n",
        "\n",
        "QR factorization is a matrix decomposition technique that factorizes a matrix into an orthogonal matrix and an upper-triangular matrix. The QR factorization of a matrix has many useful properties, such as numerical stability and orthogonality, which make it a valuable tool in many applications. QR factorization can also be utilized in order to calculate the inverses of matrices with a relatively simple algorithm."
      ],
      "metadata": {
        "id": "rDRuPegILCAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Method**\n",
        "\n",
        "The algorithms we will employ in this lab are those for calculating a sparse matrix vector product, QR-factorization of real, quadratic and invertible matrices, and using the QR-factorization of a matrix $A$ to directly solve the problem $x=A^{-1}b$.\n",
        "\n",
        "First, we implement the algorithm for the sparse matrix-vector product using the CRS format. Here, we avoid calculating the products of the zero elements in the matrix, saving computational time and making the input size smaller to store."
      ],
      "metadata": {
        "id": "LQecyV4EJiz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_matrix_vector_product(val, col_idx, row_ptr, x):\n",
        "  n = len(x)\n",
        "  b = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    for j in range(row_ptr[i]-1, row_ptr[i+1]-1):\n",
        "      b[i] += val[j]*x[col_idx[j]-1]\n",
        "  return b"
      ],
      "metadata": {
        "id": "Xvr6t3Swtqk5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we implement the algorithm for calculating QR factorization of a full rank matrix $A$. This algorithm uses a modified version of the Gramâ€“Schmidt process in order to construct an orthogonal set of vectors for the column space of $Q$, while calculating the corresponding upper-triangular matrix $R$ in parallel. These fulfill the criteria $QR = A$."
      ],
      "metadata": {
        "id": "Kro24VAwpz_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QR_factorization(A):\n",
        "  n = len(A)\n",
        "  R = np.zeros((n, n))\n",
        "  Q = np.zeros((n, n))\n",
        "  v = A[:,0]\n",
        "  for i in range(n):\n",
        "    Q[i,0] = v[i]/np.linalg.norm(v)\n",
        "  R[0,0] = np.linalg.norm(v)\n",
        "  for j in range(1, n):\n",
        "    v = A[:,j].copy()\n",
        "    v_copy = v.copy()\n",
        "    for i in range(j):\n",
        "      qi = Q[:,i]\n",
        "      R[i,j] = np.dot(qi, v_copy)\n",
        "      v = np.subtract(v, np.multiply(R[i,j], qi))\n",
        "    R[j,j] = np.linalg.norm(v)\n",
        "    for index in range(n):\n",
        "      Q[index,j] = v[index]/R[j,j]\n",
        "  return Q, R"
      ],
      "metadata": {
        "id": "R_V0y5hkuFY9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we implement the algorithm for computing the solution $x=A^{-1}b$ for a matrix $A$ of full rank. To this end, we can make use of QR factorization, since $A = QR \\implies A^{-1} = (QR)^{-1} = R^{-1}Q^{T}$. The transpose of $Q$ is easy to compute, and the upper-triangular nature of $R$ means that we can easily find the solution to $x=R^{-1}(Q^{T}b)$ using back-substitution. We can solve for $x$ by solving the equations for $x_n$, then $x_{n-1}$ and so on until we find $x_1$."
      ],
      "metadata": {
        "id": "8mgsO2mttq0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def direct_solve(A, b):\n",
        "  Q, R = QR_factorization(A)\n",
        "  n = len(b)\n",
        "  x = np.matmul(np.transpose(Q), b)\n",
        "  x_copy = x.copy()\n",
        "  x[n-1] = x[n-1]/R[n-1,n-1]\n",
        "  for i in range(n-2, -1, -1):\n",
        "    sum = 0\n",
        "    for j in range(i+1, n):\n",
        "      sum += R[i,j]*x[j]\n",
        "    x[i] = (x[i] - sum)/R[i,i]\n",
        "  return x"
      ],
      "metadata": {
        "id": "ZZCG1RzRuOsY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**\n",
        "\n",
        "We test the algorithms presented in the methods section below. We start by showing that we can calculate the sparse matrix vector product between the arbitrary matrix\n",
        "\n",
        "$\n",
        "A = \n",
        "\\left[\n",
        "  \\begin{array}{ccccc}\n",
        "    3 & 1 & 0 & 0 & 4\\\\\n",
        "    0 & 1 & 5 & 0 & 0 \\\\\n",
        "    0 & 9 & 0 & 0 & 0 \\\\\n",
        "    0 & 0 & 0 & 0 & 0 \\\\\n",
        "    0 & 2 & 6 & 0 & 5 \\\\\n",
        "  \\end{array}\n",
        "\\right]\n",
        "$\n",
        "\n",
        "and the vector\n",
        "\n",
        "$\n",
        "x = \n",
        "\\left[\n",
        "  \\begin{array}{c}\n",
        "    1\\\\\n",
        "    2\\\\\n",
        "    3\\\\\n",
        "    4\\\\\n",
        "    5\\\\\n",
        "  \\end{array}\n",
        "\\right].\n",
        "$\n",
        "\n",
        "Using the CRS format, we express $A$ as\n",
        "\n",
        "$val = [3, 1, 4, 1, 5, 9, 2, 6, 5]$, \n",
        "$col\\_idx = [1, 2, 5, 2, 3, 2, 2, 3, 5]$, and $row\\_ptr = [1, 4, 6, 7, 7, 10].$\n",
        "\n",
        "We then compare this to the result yielded by the dense matrix-vector product."
      ],
      "metadata": {
        "id": "61Vc4MOQMsmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val = [3, 1, 4, 1, 5, 9, 2, 6, 5] \n",
        "col_idx = [1, 2, 5, 2, 3, 2, 2, 3, 5] \n",
        "row_ptr = [1, 4, 6, 7, 7, 10]\n",
        "\n",
        "A = np.array([\n",
        "    [3, 1, 0, 0, 4], \n",
        "    [0, 1, 5, 0, 0], \n",
        "    [0, 9, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, 2, 6, 0, 5],\n",
        "    ])\n",
        "\n",
        "x = [1,2,3,4,5]\n",
        "\n",
        "print(\"Sparse matrix-vector product: \" + str(sparse_matrix_vector_product(val, col_idx, row_ptr, x)))\n",
        "\n",
        "print(\"Dense matrix-vector product: \" + str(np.matmul(A,x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJN7du4EVU67",
        "outputId": "75f73714-2f13-4536-d4dd-a46708090c98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparse matrix-vector product: [25. 17. 18.  0. 47.]\n",
            "Dense matrix-vector product: [25 17 18  0 47]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which shows that the sparse matrix-vector product yields the same result as the dense matrix-vector product.\n",
        "\n",
        "Next, we test the algorithm for QR factorization by applying it to the arbitrary matrix of full rank\n",
        "$\n",
        "A = \n",
        "\\left[\n",
        "  \\begin{array}{ccc}\n",
        "    3 & 1 & 4\\\\\n",
        "    1 & 5 & 9\\\\\n",
        "    2 & 6 & 5\\\\\n",
        "  \\end{array}\n",
        "\\right].\n",
        "$\n",
        "\n",
        "For the QR factorization, it should hold that $|| Q^TQ-I ||_F = 0$ and $|| QR-A ||_F = 0$, since $Q$ is orthogonal and $QR=A$"
      ],
      "metadata": {
        "id": "XURf6PPVideL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([\n",
        "    [3, 1, 4], \n",
        "    [1, 5, 9], \n",
        "    [2, 6, 5]\n",
        "    ])\n",
        "\n",
        "I = np.array([\n",
        "    [1, 0, 0], \n",
        "    [0, 1, 0], \n",
        "    [0, 0, 1]\n",
        "    ])\n",
        "\n",
        "Q, R = QR_factorization(A)\n",
        "\n",
        "A_QR = np.matmul(Q,R)\n",
        "\n",
        "I_QR = np.matmul(np.transpose(Q),Q)\n",
        "\n",
        "print(\"Matrix R: \")\n",
        "\n",
        "print(R)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"||QR - A||_F = \" + str(np.linalg.norm(A_QR - A)))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"||Q^T Q - I||_F = \" + str(np.linalg.norm(I_QR - I)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYludepM249K",
        "outputId": "b7efbd1d-15ff-4406-f369-10cae27dd296"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix R: \n",
            "[[3.74165739 5.34522484 8.2850985 ]\n",
            " [0.         5.78174467 6.00411946]\n",
            " [0.         0.         4.16025147]]\n",
            "\n",
            "||QR - A||_F = 1.9984014443252818e-15\n",
            "\n",
            "||Q^T Q - I||_F = 3.906404417713938e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can clearly see, the matrix R is upper triangular, and the Frobenius norms are both very close to zero, as expected.\n",
        "\n",
        "Finally, we test the direct solver for the matrix\n",
        "\n",
        "$\n",
        "A = \n",
        "\\left[\n",
        "  \\begin{array}{ccc}\n",
        "    3 & 1 & 4\\\\\n",
        "    1 & 5 & 9\\\\\n",
        "    2 & 6 & 5\\\\\n",
        "  \\end{array}\n",
        "\\right]\n",
        "$\n",
        "\n",
        "multiplied by the vector\n",
        "\n",
        "$\n",
        "x = \n",
        "\\left[\n",
        "  \\begin{array}{c}\n",
        "    1\\\\\n",
        "    2\\\\\n",
        "    3\\\\\n",
        "  \\end{array}\n",
        "\\right]\n",
        "$\n",
        "\n",
        "which should yield the solution\n",
        "\n",
        "$\n",
        "Ax = b =\n",
        "\\left[\n",
        "  \\begin{array}{c}\n",
        "    17\\\\\n",
        "    38\\\\\n",
        "    29\\\\\n",
        "  \\end{array}\n",
        "\\right]\n",
        "$"
      ],
      "metadata": {
        "id": "2rDTPzODPgI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([\n",
        "    [3, 1, 4], \n",
        "    [1, 5, 9], \n",
        "    [2, 6, 5]\n",
        "    ])\n",
        "\n",
        "x = [1, 2, 3]\n",
        "\n",
        "b = np.matmul(A,x)\n",
        "\n",
        "x_calculated = direct_solve(A, b)\n",
        "\n",
        "Ax = np.matmul(A,x_calculated)\n",
        "\n",
        "print(\"||Ax - b|| = \" + str(np.linalg.norm(Ax - b)))\n",
        "\n",
        "print(\"||x - y|| = \" + str(np.linalg.norm(x - x_calculated)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rhsUeX8PyvN",
        "outputId": "13e5008c-be84-43db-f7af-a9387b7ae003"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||Ax - b|| = 1.0658141036401503e-14\n",
            "||x - y|| = 2.9790409838967277e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the direct solver managed to find a very close approximation of the vector x, as expected."
      ],
      "metadata": {
        "id": "lBpWNItpPWBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discussion**\n",
        "\n",
        "As expected, we were able to find good approximate solutions to the various problems. We used the CRS format for storing a matrix and calculating a vector-matrix product, and we used an algorithm for finding QR factorizations, which we then used in order to calculate an inverse of a matrix.\n",
        "\n",
        "Observe that the CRS format is only more efficient for matrices with many zero elements, as densely populated matrices will require more space if stored in this way.\n",
        "\n",
        "Also, the QR factorization algorithm will not always produce orthogonal matrices $Q$ if the matrix $A$ is singular. Below is an example for the singular matrix\n",
        "\n",
        "$\n",
        "A = \n",
        "\\left[\n",
        "  \\begin{array}{ccc}\n",
        "    1 & 2 & 3\\\\\n",
        "    4 & 5 & 6\\\\\n",
        "    7 & 8 & 9\\\\\n",
        "  \\end{array}\n",
        "\\right]\n",
        "$\n",
        "\n",
        "which yields a non-orthogonal matrix $Q$."
      ],
      "metadata": {
        "id": "5ju8mgPxwBmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([\n",
        "    [1, 2, 3], \n",
        "    [4, 5, 6], \n",
        "    [7, 8, 9]\n",
        "    ])\n",
        "\n",
        "I = np.array([\n",
        "    [1, 0, 0], \n",
        "    [0, 1, 0], \n",
        "    [0, 0, 1]\n",
        "    ])\n",
        "\n",
        "Q, R = QR_factorization(A)\n",
        "\n",
        "I_QR = np.matmul(np.transpose(Q),Q)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"||Q^T Q - I||_F = \" + str(np.linalg.norm(I_QR - I)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "holAGNhr9f3z",
        "outputId": "074a7bfb-be0b-4d30-8bcc-cacb992fcae0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "||Q^T Q - I||_F = 1.4142135623730951\n"
          ]
        }
      ]
    }
  ]
}