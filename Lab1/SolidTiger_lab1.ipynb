{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 1: matrix factorization**\n",
        "**Jesper Lidbaum**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "In this report, three algorithms are implemented from pseudo-code provided in chapter 5 in [Johan Hoffman, Methods in Computational Science](https://my.siam.org/Store/Product/viewproduct/?ProductId=39300058). Lastly, two of the algorithms are combined to make a solver for systems of linear equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is \n",
        "\n",
        "---\n",
        "\n",
        "distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3ca850e9-08ce-48da-91a5-7bd81d6c9a27"
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2023 Jesper Lidbaum (jlidbaum@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common \n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "    \n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**\n",
        "The algorithms in this lab report are all based on pseudo code that can be found in [Johan Hoffman, Methods in Computational Science](https://my.siam.org/Store/Product/viewproduct/?ProductId=39300058). The statements about the complexity and implementation details are also all derived from there. The last task is however not entirely from the course book. But it can be derived from it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "## Sparse matrix-vector multiplication\n",
        "A problem with multiplying large matrices with vectors is that an algorithm for dense matrices has a complexity of $\\mathcal O(nÂ²)$. But this can be improved when dealing with a sparse matrix, which means a matrix that has a lot of zeros. We use a special data structure when storing sparse matrices that contain 3 arrays. One with the non-zero elements, one for their column, and an array containing pointers to where the rows begin and end in the first array. This implementation is useful because it is unnecessary to store many elements that are zero. \n",
        "\n",
        "## QR factorization\n",
        "QR factorization is a method of factorizing a matrix real, quadratic, and invertible matrix into an orthogonal and upper triangular matrix. This report is done with the modified Gram-Schmidt method. The code is based on algorithm 5.3 from the course book. This factorization is useful because an orthogonal matrix is easily inverted by transposing it, and an upper triangular matrix can be inverted by substitution. This then provides an inverse for A.\n",
        "\n",
        "## Direct solving\n",
        "As mentioned before we can generate an inverse for a quadratic and real matrix $A$ by applying the $QR$ factorization and then inverting the result. This can be used to solve equations on the form $Ax=b$. The eqation can be rewritten as $QRx=b <=> x=R^{-1}Q^{-1}b$. And since Q is orthogonal this can further be rewritten as $x=R^{-1}Q^Tb$. R is upper triangular which can be calculated with backward substitution. This is implemented in the report with algorithm 5.2 from the course book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "In this section, the code used to solve the different tasks are presented.\n",
        "\n",
        "##Algortihms\n",
        "\n",
        "### Sparse matrix-vector multiplication\n",
        "sparse_matrix_vector_product is a function that takes a vector and a sparse matrix. The sparse matrix data structure is defined with 3 arrays. First the non-zero values, then the index that the values are on in their column, and lastly an array of pointers to which value each row begins on. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation of algorithm 5.9 from the course book. Chapter 5, page 101\n",
        "def sparse_matrix_vector_product(x, val, col_idx, row_ptr):\n",
        "    b = np.zeros(len(row_ptr) - 1)\n",
        "    for i in range(len(row_ptr) - 1):\n",
        "        for j in range(row_ptr[i], row_ptr[i + 1]):\n",
        "            b[i] = b[i] + val[j] * x[col_idx[j]]\n",
        "\n",
        "    return b"
      ],
      "metadata": {
        "id": "UbevcFtasPok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QR factorization\n",
        "modified_gram_schmidt_iteration is based on algorithm 5.3 from the course book. The algorithm takes a matrix $A$ and factorizes it into the orthogonal matrix $Q$ and the upper triangular matrix $R$."
      ],
      "metadata": {
        "id": "nUtGi70Ywcko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation of algorithm 5.3 from the course book. Chapter 5, page 89\n",
        "def modified_gram_schmidt_iteration(A):\n",
        "    Q = np.zeros(A.shape)\n",
        "    R = np.zeros(A.shape)\n",
        "    v = np.zeros(A.shape[0])\n",
        "    for j in range(A.shape[1]):\n",
        "        v[:] = A[:,j].copy()\n",
        "        for i in range(j):\n",
        "            R[i,j] = np.dot(Q[:,i], v[:])\n",
        "            v[:] = v[:] - R[i,j] * Q[:,i]\n",
        "        R[j,j] = np.linalg.norm(v)\n",
        "        Q[:,j] = np.multiply(v[:], 1.0/R[j,j])\n",
        "    return Q, R"
      ],
      "metadata": {
        "id": "Ca3w52OHw_Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct solving\n",
        "The direct solver uses the $QR$ decomposition gained from modified_gram_schmidt_iteration. It then solves the equation $Ax=b$ by calculating the value of $x=R^{-1}Q^Tb$ in two steps. First, the transpose of $Q$ is multiplied by b. We let $y=Q^Tb$. Then $Rx=y$ is solved using backward subsitution. "
      ],
      "metadata": {
        "id": "Bd7vCOCqxGOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation of algorithm 5.2 from the course book. Chapter 5, page 87\n",
        "def backward_substitution(R, b):\n",
        "    n = R.shape[0]\n",
        "    x = np.zeros(n)\n",
        "    x[n-1] = b[n-1] / R[n-1, n-1]\n",
        "    for i in range(n-2, -1, -1):\n",
        "        sum = 0\n",
        "        for j in range(i+1, n):\n",
        "            sum = sum + R[i,j] * x[j]\n",
        "        x[i] = (b[i] - sum) / R[i,i]\n",
        "    return x\n",
        "\n",
        "def direct_solver(A, b):\n",
        "    Q, R = modified_gram_schmidt_iteration(A)\n",
        "    Qt = np.transpose(Q)\n",
        "    y = Qt.dot(b)\n",
        "    x = backward_substitution(R, y)\n",
        "    return x"
      ],
      "metadata": {
        "id": "gmw9pQlLzYsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests\n",
        "Here we define some test methods. The tests are from the lab instructions. \n",
        "\n",
        "### Sparse matrix-vector multiplication\n",
        "We test the equation $\\left(\\begin{array}{ccc} \n",
        "1 & 2 & 3\\\\\n",
        "4 & 5 & 6\\\\\n",
        "7 & 8 & 9\\\\\n",
        "\\end{array}\\right)\n",
        "\\left(\\begin{array}{c} \n",
        "1\\\\\n",
        "2\\\\\n",
        "3\n",
        "\\end{array}\\right)$ with our implementation against numpy's implementation of dense multiplication. The residual norm between the results is returned to indicate the difference."
      ],
      "metadata": {
        "id": "te7SjbNu4Zyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_sparse_matrix_vector_product():\n",
        "    A = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
        "    val = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "    col_idx = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n",
        "    row_ptr = np.array([0, 3, 6, 9])\n",
        "    x = np.array([1, 2, 3])\n",
        "    b = sparse_matrix_vector_product(x, val, col_idx, row_ptr)\n",
        "    return (np.linalg.norm(b - A.dot(x)))"
      ],
      "metadata": {
        "id": "rgb_17P17XJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QR factorization\n",
        "Similarly, we check the result of the factorization. $Q^TQ$ should be equal to the identity matrix. And $QR$ should be equal to $A$. We test the result with the Forbenius norm which is the default for [numpy norms](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html). "
      ],
      "metadata": {
        "id": "hc5UdjEC7Z0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_modified_gram_schmidt_iteration():\n",
        "    A = np.array([[2, -1], [-1, 2]])\n",
        "    Q, R = modified_gram_schmidt_iteration(A)\n",
        "    return (np.linalg.norm(Q.dot(R) - A)), (np.linalg.norm(Q.dot(np.transpose(Q)) - np.identity(2)))"
      ],
      "metadata": {
        "id": "-eca5s_d-RLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Direct solving\n",
        "We test the direct solving with the residual norm between a fabricated result. This with numpy's solver and our own."
      ],
      "metadata": {
        "id": "y_-pmTlL-mdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_direct_solver():\n",
        "    A = np.array([[2, -1], [-1, 2]])\n",
        "    b = np.array([100, 10])\n",
        "    x = direct_solver(A, b)\n",
        "    y = np.linalg.solve(A, b)\n",
        "    return (np.linalg.norm(A.dot(x) - b)), (np.linalg.norm(x - y))"
      ],
      "metadata": {
        "id": "ZcJErow7_R7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "We present the results from the different tests in this section.\n",
        "\n",
        "### Sparse matrix-vector multiplication\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_sparse_matrix_vector_product())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjaOGdWN_icw",
        "outputId": "b9ef351d-a10a-4666-bac6-74edf56b953b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QR factorization"
      ],
      "metadata": {
        "id": "L38jFkp7_p0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_modified_gram_schmidt_iteration())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj200nSp_xYb",
        "outputId": "45eeb108-9397-4d62-d272-8f19d40b0d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.0, 3.1836122848239643e-16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test direct solving"
      ],
      "metadata": {
        "id": "qnttE8PE_07r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_direct_solver())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyK5AsD__2uY",
        "outputId": "8cd9bcc2-3e54-42fd-e3ca-8c53a4a8c318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1.4210854715202004e-14, 7.105427357601002e-15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The conclusion is that the implemented algorithms seems to work for the tests. All the norms are either very small or 0. This is however no proof of correctness. But these algorithms are implementations of psuedo code that is well researched and a correct implementation should work. I was suprised at how well the direct solver worked, it had a simple implementation.\n",
        "\n",
        "The psuedo code was also written in a way that was easily translated to python. However the differences in starting index is something that you need to be aware of."
      ]
    }
  ]
}