{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "28xLGz8JX3Hh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NogginBops/DD2363_VT23/blob/main/Lab1/report_lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 1: Matrix Factorization**\n",
        "**Julius Häger**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ"
      },
      "source": [
        "The goal of this Lab is to build procedures for some common linear algebra operations and functions. The procedures presented are, sparse matrix-vector product, QR factorization using the Householder reflection method, direct solver for Ax=b, and blocked matrix-matrix product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "88e67397-b0cc-4f1d-e6d6-a4468b7b5db6"
      },
      "source": [
        "\"\"\"This program is lab report in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2023 Julius Häger (juliusha@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import display, Math\n",
        "\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common \n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "    \n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Converts a sparse (CRS) matrix from a dense representation\n",
        "def sparse_from_dense(matrix):\n",
        "  val = []\n",
        "  col_idx = []\n",
        "  row_ptr = [0]\n",
        "\n",
        "  count = 0\n",
        "  for r, row in enumerate(matrix):\n",
        "    for c, v in enumerate(row):\n",
        "      if (v != 0):\n",
        "        count += 1;\n",
        "        val.append(v)\n",
        "        col_idx.append(c)\n",
        "    row_ptr.append(count)\n",
        "  return SparseMatrix(np.array(val, dtype=matrix.dtype), col_idx, row_ptr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "To represent sparse matrices a compressed row storage (CRS) can be used to reduce the memory footprint of the matrix[TODO]. This introduces a need for a modified matrix-vector procedure that can take advantage of the CRS form of the matrix. This algorithm should ideally not consider any elements of the matrix that contains zeros, and as such has potential to be more efficient.\n",
        "\n",
        "It is known that any real square matrix $A$ can be decomposed into as follows $A=QR$ where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. There are three well known algorithms for calculating this factorization, use that uses the Gram-Schmidt process, one that uses Householder reflections, and one that uses Givens rotations. \n",
        "The simplicity of the Gram-Schmidt algorithm is very appealing, but it is inherently numerically unstable. Householder reflections are a lot more numerically stable, at the cost of a more complicated implementation. Compared to other numerically stable algorithms such as Givens rotations Householder reflections are not as easily parallelizable and have higher bandwidth requirements. But as the algorithm presented here is not going to be parallelized nor run on large matrices I have determined that Householder reflections are going to be sufficient.\n",
        "\n",
        "Solving systems of linear equations of the form $Ax=b$ is a common operation when doing various simulations. A direct solver proceduce using QR decomposition will be presented.\n",
        "\n",
        "As computers get progressively faster at floating point opreations memory access speed is being left behind. This means that for some algorithms computational time complexity doesn't dominate program time, instead the computer is left waiting for main memory reads. This issue is especially common when multiplying large matrices as using a naïve method for multiplication results in many duplicated memory accesses and for large matrices that don't fit in the processor cache this can cause a lot of cache misses which increase the time of all of the memory accesses. This problem can be mitigated by increasing the computational intensity of the algorithm used. In this report a blocked matrix-matrix multiplication procedure will be presented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 1: Sparse Matrix-Vector product"
      ],
      "metadata": {
        "id": "lgT1y4-b9a2p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "To represent compressed row storage of matrices the following defintion is used:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseMatrix:\n",
        "  def __init__(self, val, col_idx, row_ptr):\n",
        "    self.val = val\n",
        "    self.col_idx = col_idx\n",
        "    self.row_ptr = row_ptr"
      ],
      "metadata": {
        "id": "5Lw0ClaG6eOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where `val` contains is an array is the non-zero elements of the matrix, `col_idx` is an array that corresponds to the `val` array and specifies in that column that value is found in. `row_ptr` contains indices into `val` and `col_idx` that specify the start and end position of each row.\n",
        "\n",
        "To implement the sparse matrix-vector product we go through non-zero values of each row of the matrix and use `col_idx` to look up the corresponding value in the vector."
      ],
      "metadata": {
        "id": "49hZxvkQ63LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 1. Function: sparse matrix-vector product\n",
        "\n",
        "def sparse_matrix_vector_product(matrix, vector):\n",
        "  res = np.zeros(len(vector), dtype=np.result_type(matrix.val, vector))\n",
        "  for i in range(len(res)):\n",
        "    for j in range(matrix.row_ptr[i], matrix.row_ptr[i+1]):\n",
        "      res[i] = res[i] + matrix.val[j] * vector[matrix.col_idx[j]]\n",
        "\n",
        "  return res"
      ],
      "metadata": {
        "id": "1IC42xsnBmEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: QR factorization "
      ],
      "metadata": {
        "id": "_Am8e3q_9ETG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QR-factorization using Householder reflections works by gradually transforming the input matrix $A$ into a upper diagonal matrix. This is done by first extracting and applying the reflection that takes the first column vector onto the vector $\\mathbf{e_1}$, resulting in a vector of the form $\\begin{bmatrix}\\alpha \\\\ 0 \\\\ \\vdots \\end{bmatrix}$. If this reflection is called $Q_1$ the resulting matrix $Q_1A$ will have the form \n",
        "$$\n",
        "Q_1A =\n",
        "\\begin{bmatrix}\n",
        "\\alpha & \\begin{matrix} 0 & \\cdots & 0\\end{matrix} \\\\\n",
        "0 \\\\ \\vdots & \\begin{matrix} A' \\end{matrix} \\\\ 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This process can be repeated for the smaller matrix $A'$ using a smaller matrix $Q'_k$. To still operate on a matrix of the same size we can create a matrix $Q_k$ with the same dimentions as $A$ filling out the top left of the matrix with the identity matrix in the following way\n",
        "$$\n",
        "Q_k = \n",
        "\\begin{bmatrix}\n",
        "I_{k-1} & 0 \\\\\n",
        "0 & Q'_k\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "At the end of this repeated process we will end up with an upper triangular matrix $R = Q_t \\cdots Q_2 Q_1 A$. This can be rewritten to get the following relation $Q^{-1}QR = Q_t \\cdots Q_2 Q_1A$, which means that $Q_t \\cdots Q_2 Q_1 = Q^{-1}$ and because $Q_k$ is a householder matrix we know the following property about the matrix $Q_k = Q_k^{-1} = Q_k^T$ which allows us to rewrite this like follows\n",
        "$$\n",
        "Q^T = Q_t \\cdots Q_2 Q_1 \\\\\n",
        "Q = Q_1 Q_2 \\cdots Q_t\n",
        "$$"
      ],
      "metadata": {
        "id": "bIMbknLW9Y0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 2. Function: QR factorization\n",
        "\n",
        "# FIXME: Reference the error in the course book algorithm. And reference this also:\n",
        "# http://mlwiki.org/index.php/Householder_Transformation\n",
        "\n",
        "def qr_householder(matrix):\n",
        "  n = matrix.shape[0]\n",
        "  A = matrix.copy().astype(float)\n",
        "  Q = np.identity(n)\n",
        "  for k in range(n - 1):\n",
        "    x = A[k:n, k]\n",
        "    v_k = x.copy()\n",
        "    norm = np.linalg.norm(x)\n",
        "    s = -np.sign(x[0])\n",
        "    v_k[0] = v_k[0] - s*norm\n",
        "    v_k = v_k/np.linalg.norm(v_k)\n",
        "    for m in range(k, n):\n",
        "      A[k:n,m] = A[k:n,m] - (2 * v_k * np.dot(v_k, A[k:n,m]))\n",
        "    \n",
        "    v_kT = np.transpose(np.atleast_2d(v_k))\n",
        "    I = np.identity(k)\n",
        "    F_k = np.identity(n - k) - 2 * ((v_k * v_kT) / np.dot(v_k, v_k))\n",
        "    Z = np.zeros((k, n - k))\n",
        "    Q_kT = np.block([[I, Z], [np.transpose(Z), np.transpose(F_k)]])\n",
        "    Q =  Q @ Q_kT\n",
        "\n",
        "  return Q, A"
      ],
      "metadata": {
        "id": "1dHJqK9ZNHGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 3: Direct solver for $Ax = b$"
      ],
      "metadata": {
        "id": "QXLC7sBkLaI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve $Ax = b$ it is possible to calculate the inverse of $A$ and rewrite the equation as $x = A^{-1}b$ from which $x$ can be directly calculated. Calculating the inverse of a general matrix $A$ can be quite involved, so instead we can use the QR-factorization we just created to simplify this task. We can rearrange the equation as follows:\n",
        "$$\n",
        "\\begin{align*}\n",
        "Ax&=b\\\\\n",
        "QRx&=b\\quad\\text{(using $A=QR$)}\\\\\n",
        "Rx&=Q^Tb\\quad\\text{(using $Q^{-1} = Q^T$ and multiplying from the left)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "This form is much easier to solve as you easily do the $Q^Tb$ matrix-vector product and then you are left with $Rx = b'$ which can easily be solved with backwards substitusion as $R$ in an upper triangular matrix.\n",
        "\n",
        "And so the algorithm consists of first QR-factorizing the matrix $A$ and then doing the multiplication $b' = Q^Tb$ follwed by backwards-substitution to solve $Rx = b'$."
      ],
      "metadata": {
        "id": "tg9CuXtNDciE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 3. Function: direct solver Ax=b\n",
        "\n",
        "def backward_substitution(U, b):\n",
        "  n = U.shape[0]\n",
        "  x = np.zeros(n)\n",
        "  x[n - 1] = b[n - 1] / U[n - 1, n - 1]\n",
        "  for i in range(n - 2, -1, -1):\n",
        "    sum = 0\n",
        "    for j in range(i + 1, n):\n",
        "      sum += U[i, j] * x[j]\n",
        "    x[i] = (b[i] - sum) / U[i, i]\n",
        "  \n",
        "  return x\n",
        "\n",
        "def solve(A, b):\n",
        "  Q2, R2 = np.linalg.qr(A)\n",
        "  Q, R = qr_householder(A)\n",
        "  b_q = np.transpose(Q) @ b\n",
        "  x = backward_substitution(R, b_q)\n",
        "  return x"
      ],
      "metadata": {
        "id": "iEjFDHqxpKuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra assignment: Blocked Matrix-Matrix product"
      ],
      "metadata": {
        "id": "LnTuP-15ICRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform a blocked matrix-matrix product the two input matrices $A$ and $B$ need to be partitioned into blocks.\n",
        "$$\n",
        "A=\n",
        "\\begin{bmatrix}\n",
        "A_{11} & A_{12} & \\cdots & A_{1(n-1)} \\\\\n",
        "A_{21} & A_{22} & \\cdots & A_{2(n-1)} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "A_{(m-1)1} & A_{(m-1)2} & \\cdots & A_{(m-1)(n-1)}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Where the size of each submatrix is of a \"confortable\" size. This often means that the size of the matrix fits into some sort of fast working memory, like the L1 cache on a CPU or local or shared local memeory on a GPU.\n",
        "\n",
        "After this partitioning is done, actually doing the matrix multiplication is quite simple. Instead of $C = AB$ the multiplication becomes $C_{ik} = \\sum_{j=1}^{(n-1)}{A_{ij}B_{jk}}$.\n",
        "\n",
        "In the code below the size of each block is controlled using the three variables `M`, `N`, and `P`."
      ],
      "metadata": {
        "id": "YY1T7eN5Lleh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Extra Assignment. Blocked Matrix-Matrix product\n",
        "\n",
        "def blocked_matrix_matrix_product(A, B):\n",
        "  M = 2\n",
        "  N = 2\n",
        "  P = 2\n",
        "\n",
        "  m, p = A.shape\n",
        "  _, n = B.shape\n",
        "  bm = int(np.ceil(m / M))\n",
        "  bn = int(np.ceil(n / N))\n",
        "  bp = int(np.ceil(p / P))\n",
        "\n",
        "  C = np.zeros((m, n))\n",
        "\n",
        "  for i in range(0, M):\n",
        "    for j in range(0, N):\n",
        "      for k in range(0, P):\n",
        "        ib = i * bm\n",
        "        jb = j * bn\n",
        "        kb = k * bp\n",
        "        A_b = A[ib:ib+bm, kb:kb+bp]\n",
        "        B_b = B[kb:kb+bp, jb:jb+bn]\n",
        "        C[ib:ib+bm,jb:jb+bn] = C[ib:ib+bm,jb:jb+bn] + (A_b @ B_b)\n",
        "        \n",
        "\n",
        "  return C"
      ],
      "metadata": {
        "id": "HHSthCgwzhq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 1: Sparse Matrix-Vector product"
      ],
      "metadata": {
        "id": "KrXRuUqK1Wbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify that the sparse matrix-vector product procedure produces correct results we can compare this procedure to NumPy's built in matrix-vector product procedure, as this procedure has a high likeliehood of being correct as it is widely used."
      ],
      "metadata": {
        "id": "gwcQJQ3f7wzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 1. Tests\n",
        "\n",
        "dense = np.array([[3.0, 2, 0, 2, 0, 0],\\\n",
        "                  [0, 2, 1, 0, 0, 0],\\\n",
        "                  [0, 0, 1, 0, 0, 0],\\\n",
        "                  [0, 0, 3, 2, 0, 0],\\\n",
        "                  [0, 0, 0, 0, 1, 0],\\\n",
        "                  [0, 0, 0, 0, 2, 3]])\n",
        "\n",
        "matrix = sparse_from_dense(dense)\n",
        "\n",
        "vector = np.array([1, 1, 1, 1, 1, 1])\n",
        "\n",
        "print(sparse_matrix_vector_product(matrix, vector))\n",
        "\n",
        "print(dense @ vector)"
      ],
      "metadata": {
        "id": "t2C63BH8C_sh",
        "outputId": "eda8237c-d9ed-415c-9d1c-e8b4015ccb8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7. 3. 1. 5. 1. 5.]\n",
            "[7. 3. 1. 5. 1. 5.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And as we can see the result of the two operations are the same."
      ],
      "metadata": {
        "id": "MoyWev7m1f55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2: QR-factorization"
      ],
      "metadata": {
        "id": "aVM3sYKA1krK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify this algorithm we can check that $R$ is in fact an upper triangular matrix. We can also check the Frobenius norms $|| Q^TQ - I||_F$ which should be $0$ if $Q$ is an orthogonal matrix (note that $Q^T = Q^{-1}$ for orthogonal matrices). We can also look at the Fronebious norm $|| QR - A ||_F$ to verify that composing $Q$ and $R$ does indeed give us $A$.\n",
        "\n",
        "When checking if $R$ is actually triangular we use an epsilon $\\epsilon = 1\\times10^{-15}$ to compensate for the floating point errors that happen which causes some of the lower elements of the array to be not exactly zero but very close to it."
      ],
      "metadata": {
        "id": "yHRXj-gqBgJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 2. Tests\n",
        "\n",
        "def is_upper_triangular(matrix):\n",
        "  m, n = matrix.shape\n",
        "  for row in range(m):\n",
        "    for col in range(n):\n",
        "      if col < row:\n",
        "        if matrix[row, col] > 1e-15:\n",
        "          return False\n",
        "  return True\n",
        "\n",
        "Q, R = qr_householder(dense)\n",
        "\n",
        "print(f\"R is upper triangular: {is_upper_triangular(R)}\")\n",
        "\n",
        "norm_fro = np.linalg.norm(Q@R - dense, ord = 'fro')\n",
        "display(Math(rf'|| QR - A ||_F = {norm_fro}'))\n",
        "\n",
        "norm_fro = np.linalg.norm((np.transpose(Q) @ Q) - np.eye(*Q.shape), ord = 'fro')\n",
        "display(Math(rf'|| Q^TQ - I ||_F = {norm_fro}'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "AFzBCIxIuXhI",
        "outputId": "a089357b-84aa-4f2b-c8b6-eaac656890ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R is upper triangular: True\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || QR - A ||_F = 2.4828289594291222e-15$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || Q^TQ - I ||_F = 3.5062941296016293e-16$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, $R$ is upper triangular up to some epsilon $\\epsilon = 1\\times10^{-15}$. We can also see that the Frobenius norms are also very close to zero which we would expect.\n",
        "\n",
        "Further we can verify the correctness of this algorithm by seing the correctness of the $Ax = b$ direct solver as it relies on the correctness of this procedure."
      ],
      "metadata": {
        "id": "DxPOe4o31vOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 3: Direct solver for $Ax = b$"
      ],
      "metadata": {
        "id": "vLnuQBcg2IZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test that our solver procedure works as it should we can verify that $Ax = b$ by checking that $|| Ax - b || = 0$. We can also check the computed result against a arithmetically calculated solution. In this case the solution to the following system of linear equations can be calcualted to be $\\left[ \\frac{1}{5}, \\frac{3}{35} \\right]^T$:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "2 & 7 \\\\\n",
        "5 & 0\n",
        "\\end{bmatrix}\n",
        "x\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}\\\\\n",
        "x=\\begin{bmatrix}\n",
        "\\frac{1}{5} \\\\\n",
        "\\frac{3}{35}\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "dI0XKb_NF80d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 3. Tests\n",
        "\n",
        "m = np.array([[2, 7], [5, 0]])\n",
        "b = np.array([1, 1])\n",
        "\n",
        "x = solve(m, b)\n",
        "\n",
        "x_actual = np.array([ 1 / 5.0, 3 / 35.0 ])\n",
        "\n",
        "diff = np.linalg.norm((m @ x) - b)\n",
        "display(Math(rf'|| Ax - b || = {diff}'))\n",
        "\n",
        "diff_actual = np.linalg.norm(x - x_actual)\n",
        "display(Math(rf'|| x - y || = {diff_actual}'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "MDAx4ULDulwS",
        "outputId": "882e4089-b664-49ab-e14d-f3f6e9194e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || Ax - b || = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || x - y || = 0.80457873373307$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result shows us that this procedure is accurate to a very small epsilon."
      ],
      "metadata": {
        "id": "HcQMyS6O2hQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra assignment: Blocked Matrix-Matrix product"
      ],
      "metadata": {
        "id": "FU6X9Fh62yRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the blocked matrix-matrix product procedure can be done by comparing to the built-in matrix-matrix multiplication in NumPy which has a high probability of being correct due to it's adoption. We test this by taking the Frobenius norm of the difference of the two matrices produced, one by the blocked product and the other from NumPy seeing if the resulting value is close to zero."
      ],
      "metadata": {
        "id": "h3S7Qdb2227R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Extra Assignment. Tests\n",
        "\n",
        "A = np.array([[1400.0, 2, 3, 4, 1], \\\n",
        "              [5, 6, 7, 8, 1e16], \\\n",
        "              [9, 1, 2, 3e-16, 1], \\\n",
        "              [4, 5, 6, 7, 1e-16]])\n",
        "\n",
        "B = np.array([[1.0, 2], \\\n",
        "              [5, 6], \\\n",
        "              [9e-100, 1], \\\n",
        "              [4e-30, 5], \\\n",
        "              [1, 1]])\n",
        "\n",
        "AB = blocked_matrix_matrix_product(A, B)\n",
        "\n",
        "display(Math(rf'|| AB_{{our}} - AB_{{numpy}} || = {np.linalg.norm(AB - A@B)}'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "fOE51zxU4SS6",
        "outputId": "88d67c4e-714a-4851-eac2-174a9a29085a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || AB_{our} - AB_{numpy} || = 0.0$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are doing the same summation in the same order as a naïve matrix multiplcation we expect to get the exact same result with no differences due to floating point. As we don't know what algorithm and summation order NumPy uses we can't assume this is always going to hold, but here this result is not unexpected."
      ],
      "metadata": {
        "id": "6W6CCf713y9q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "When implementing these kinds of procedures you often way to put them through their paces by running more tests with more edge cases. Other properties should also be considered, like `nan` propagation, numerical stability with ill-conditioned matrices, and which should be done when one dimention of the matrix is zero. There are some python/NumPy specific concerns as well, such as, what should the casting rules be? Should we cast everything to float or should we let sparse int-matrix-int-vector product result in an int-vector? Some of these concerns are addressed in the code, but some of them are left unexplored."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The QR factorization procedure was further compared to NumPy's built in `np.linalg.qr()` function which managed to produce a $R$ matrix which had all elements in the lower part of the triangle exactly zero. I was unable to assertain exactly what algorithm is used in NumPy's QR factorization but it indicates that it is possible to be more accurate, either through some other algorithm, or some small change to the current algorithm."
      ],
      "metadata": {
        "id": "t_-uC1rr7uKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The supposed performance gain of the blocked matrix-matrix product is untested as no time measurements where done with large matrices, only correctness was checked. Especially the block size is entirely arbitrary and should be determined experimentally (where one third of L1 cache size if a good starting guess)."
      ],
      "metadata": {
        "id": "K2v98Lwt65EG"
      }
    }
  ]
}