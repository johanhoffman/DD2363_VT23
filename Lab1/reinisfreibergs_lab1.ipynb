{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT23/tree/reinisfreibergs-Lab1/Lab1/reinisfreibergs_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RgtXlfYO_i7"
   },
   "source": [
    "# Lab 1: matrix factorization \n",
    "**Reinis Freibergs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x_J5FVuPzbm"
   },
   "source": [
    "# **Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UFTSzW7P8kL"
   },
   "source": [
    "\n",
    "The objective of the lab is to implement and test the sparse matrix-vector multiplication, QR factorisation with the modified Gram-Schmidt method, directly solving a linear system with QR substitution and backsubstitution, as well as a Least Squares problem. <br>\n",
    "All algorithms were tested with random matrices with the assumed configuration and returned the same outputs as ready made solutions from numpy within a tolerance of round-off error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkT8J7uOWpT3"
   },
   "source": [
    "# **About the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "Pdll1Xc9WP0e",
    "outputId": "9e782dc7-4ad0-48f9-d45c-2200e83dec4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
    "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
    "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
    "\n",
    "\n",
    "# Author: Reinis Freibergs, 2023\n",
    "\n",
    "# Based on a template:\n",
    "# Copyright (C) 2023 Johan Hoffman (jhoffman@kth.se)\n",
    "\n",
    "\n",
    "# This file is part of the course DD2363 Methods in Scientific Computing\n",
    "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
    "#\n",
    "# This is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Lesser General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This template is maintained by Johan Hoffman\n",
    "# Please report problems to jhoffman@kth.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28xLGz8JX3Hh"
   },
   "source": [
    "# **Set up environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2PYNusD08Wa"
   },
   "source": [
    "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "Xw7VlErAX7NS"
   },
   "outputs": [],
   "source": [
    "# Load neccessary modules.\n",
    "#from google.colab import files\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnO3lhAigLev"
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5zMzgPlRAF6"
   },
   "source": [
    "Systems of linear equations $Ax=b$ are important in the study of differential and integral equations. In case of the matrix $A$ being nonsingular the solution is given by finding the inverse of $A$:\n",
    "$$x = A^{-1}b$$\n",
    "\n",
    "In this report the implementations of important algorithms for finding the solutions of systems of linear equations are given:<br>\n",
    "1.  Sparse matrix-vector product - for the case when a product needs to be found between a matrix with most elements being zero and a vector.\n",
    "<br>\n",
    "2. Modified Gram-Schmidt QR factorization - numerically stable method to factorize a matrix in a product of two easily invertible matrices.\n",
    "<br>\n",
    "3. A direct solver with the use of QR factorization and backsubstitution.\n",
    "4. A least squares problem - for the case of overdetermined matrices\n",
    "\n",
    "<br>\n",
    "All implementations are based on materials from the lecture notes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOQvukXZq5U5"
   },
   "source": [
    "# **Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrix-vector product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If most components of a matrix are zero it is sparse. In that case a regular dense multiplication involves mostly multiplying by zero and is wasteful. To tackle the problem a specific data structure called _compressed raw storage_ (CRS) can be used, where the matrix is represented with three arrays:\n",
    "1. non-zero values _val_\n",
    "1. respective column indices _col_idx_\n",
    "1. array pointing to the start of each array _row_ptr_\n",
    "\n",
    "The implementation itself is based on the algorithm 5.9 in lecture notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_product(vector, val, col_idx, row_ptr):\n",
    "    \n",
    "    product = np.zeros(shape=len(row_ptr) - 1)\n",
    "    for i in range(len(row_ptr) - 1):\n",
    "        for j in range(row_ptr[i], row_ptr[i + 1]):\n",
    "            product[i] += val[j] * vector[col_idx[j]]\n",
    "            \n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation is tested against the respective _numpy_ function. The sparce matrices are generated randomly and translated in the src format with the scipy package, as it was not an objective of the lab to implement these parts as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test no. 0, matrix_shape:(7, 8), difference: [0. 0. 0. 0. 0. 0. 0.]\n",
      "Test no. 1, matrix_shape:(6, 6), difference: [0. 0. 0. 0. 0. 0.]\n",
      "Test no. 2, matrix_shape:(6, 3), difference: [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    columns = np.random.randint(2, 10)\n",
    "    rows = np.random.randint(2, 10)\n",
    "    test_matrix = sparse.random(m=rows,\n",
    "                                n=columns,\n",
    "                                density=0.1)\n",
    "\n",
    "    csr_matrix = sparse.csr_matrix(test_matrix)\n",
    "    test_vector = np.random.rand(columns)\n",
    "    \n",
    "    dense_product = test_matrix @ test_vector\n",
    "    sparse_product = matrix_product(test_vector, csr_matrix.data, csr_matrix.indices, csr_matrix.indptr)\n",
    "    \n",
    "    print(f'Test no. {i}, matrix_shape:{rows, columns}, difference: {dense_product - sparse_product}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR factorization allows to factor a matrix $A$ into and orthogonal matrix $Q$ and a triangular matrix $R$, where both are easy to invert - for an orthogonal matrix the inverse is given by its transpose $Q^{-1} = Q^T$ and for triangular the inverse can be found by backsubstitution, implemented in the next algorithm.\n",
    "\n",
    "Exact implementation uses the modified Gram-Schmidt algorithm, based on the algorithm 5.3 from lecture notes, as it's more numerically stable than the classical Gram-Schmidt method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_gram_schmidt_QR(A):\n",
    "  n = A.shape[0]\n",
    "  Q = np.zeros(shape=(n, n))\n",
    "  R = np.zeros_like(Q)\n",
    "  for j in range(n):\n",
    "    v_j = A[:, j]\n",
    "    for i in range(j):\n",
    "      R[i, j] = Q[:, i] @ v_j\n",
    "      v_j = v_j - R[i, j] * Q[:, i]\n",
    "\n",
    "    R[j, j] = np.linalg.norm(v_j)\n",
    "    Q[:, j] = v_j / R[j, j]\n",
    "\n",
    "  return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the implementation its checked whether $R$ is upper triangular, if $Q^TQ=I$ and $QR=A$, where for the latter two Frobenius norms are used accordingly $\\|Q^TQ -I \\|$ and $\\|QR - A \\|$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test no. 0\n",
      "is upper triangular: True\n",
      "||Q^TQ -I||: 3.9885061540178885e-16\n",
      "||QR -A||: 3.379462171739212e-16\n",
      "test no. 1\n",
      "is upper triangular: True\n",
      "||Q^TQ -I||: 5.496298777836552e-16\n",
      "||QR -A||: 4.560714706164002e-16\n",
      "test no. 2\n",
      "is upper triangular: True\n",
      "||Q^TQ -I||: 5.847987538262697e-16\n",
      "||QR -A||: 3.497780720931476e-16\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    n = np.random.randint(2, 10)\n",
    "    # add identity matrix to make sure its invertible\n",
    "    A = np.random.rand(n,n) + np.eye(n)\n",
    "\n",
    "    Q, R = modified_gram_schmidt_QR(A)\n",
    "    # R is upper triangular - sum of all indices below diagonal should be 0\n",
    "    lower_sum = (np.sum(np.tril(R, -1)) == 0)\n",
    "\n",
    "    f1 = np.linalg.norm(Q.T @ Q - np.eye(n))\n",
    "    f2 = np.linalg.norm(Q @ R - A)\n",
    "    \n",
    "    print(f'test no. {i}')\n",
    "    print(f'is upper triangular: {lower_sum}') \n",
    "    print(f'||Q^TQ -I||: {f1}')\n",
    "    print(f'||QR -A||: {f2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The direct solver aims to solve the system of linear equations $Ax=b$ by finding $x = A^{-1}b$. For this the previously implemented QR factorization can be used, as it's resulting matrices are easy to invert. In that case the inversion turns into:\n",
    "$$Ax = QRx = b$$\n",
    "$$Rx = Q^{-1}b = Q^Tb$$\n",
    "where the inverse of matrix $Q^{-1} = Q^{T}$ is given by its transpose as the property of orthogonal matrices, while the inverse of the upper triangular matrix $R$ will be found by implementing the backward substitution algorithm based on 5.2 from the lecture notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_substitution(U, b):\n",
    "    n = len(b)\n",
    "    x = np.zeros(shape=(n,))\n",
    "    x[n-1] = b[n-1] / U[n-1, n-1]\n",
    "    for i in range(n-2, -1, -1):\n",
    "        sum = 0\n",
    "        for j in range(i+1, n):\n",
    "            sum += U[i, j]*x[j]\n",
    "        x[i] = (b[i] - sum)/U[i, i]\n",
    "        \n",
    "    return x\n",
    "\n",
    "def direct_solver(A, b):\n",
    "    Q, R = modified_gram_schmidt_QR(A)\n",
    "    Q_i = Q.T\n",
    "    Q_i_b = Q_i @ b\n",
    "    x = backward_substitution(R, Q_i_b)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is tested with the residuals $\\|Ax - b \\|$ and $\\|x-y \\|$ where y is a manufactured solution with $b=Ay$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test no. 0\n",
      "||Ax - b||: 4.330225622130344e-15\n",
      "||x - y||: 2.4022911209387004e-13\n",
      "test no. 1\n",
      "||Ax - b||: 7.535012591929473e-16\n",
      "||x - y||: 5.136806496458733e-15\n",
      "test no. 2\n",
      "||Ax - b||: 5.36250363036098e-15\n",
      "||x - y||: 8.925828822937747e-13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    n = np.random.randint(1,25)\n",
    "    \n",
    "    A = np.random.rand(n, n) + np.eye(n)\n",
    "    b = np.random.rand(n)\n",
    "    \n",
    "    x1 = direct_solver(A,b)\n",
    "    residual_1 = np.linalg.norm(A @ x1 - b)\n",
    "    \n",
    "    y = np.random.rand(n)\n",
    "    b_y = A @ y\n",
    "    x_y = direct_solver(A, b_y)\n",
    "    residual_2 = np.linalg.norm(x_y - y)\n",
    "    \n",
    "    print(f'test no. {i}')\n",
    "    print(f'||Ax - b||: {residual_1}')\n",
    "    print(f'||x - y||: {residual_2}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares problem seeks the best fitting solution $\\bar{x}$ for the equation $Ax = b$ for the case of a rectangular $mxn$ matrix with $m>n$, meaning that there are more equations than unknowns, which is often used for data fitting.\n",
    "\n",
    "The solution can be found by the _pseudoinverse_ or the _Moore-Penrose_ inverse defined in example 2.17\n",
    "\n",
    "$$A^+ = (A^TA)^{-1}A^T$$\n",
    "\n",
    "$$\\bar{x} = (A^TA)^{-1}A^Tb$$\n",
    "\n",
    "While using the previously implemented QR factorization to find the inverse of $(A^TA)^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(A, b):\n",
    "    a_t_a = A.T @ A\n",
    "    a_t_b = A.T @ b\n",
    "    Q, R = modified_gram_schmidt_QR(a_t_a)\n",
    "    q_t = Q.T\n",
    "    q_t_a_t_b = q_t @ a_t_b\n",
    "    x = backward_substitution(R, q_t_a_t_b)\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the algorithm is tipically used for overdetermined systems $(m>n)$ the tests are going to use both square matrices to test against the residual of exact solution $\\|Ax - b \\|$ and overdetermined solutions with $m>n$ compared with the numpy solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test no: 0\n",
      "||Ax-b|| with m=n: 9.585647399488852e-13\n",
      "||Ax-b|| with m>n: 1.855911558098565\n",
      "numpy solution ||Ax - b ||: with m>n: 1.8559115581019565\n",
      "test no: 1\n",
      "||Ax-b|| with m=n: 1.1102230246251565e-16\n",
      "||Ax-b|| with m>n: 2.1822396537558912\n",
      "numpy solution ||Ax - b ||: with m>n: 2.1822396537558912\n",
      "test no: 2\n",
      "||Ax-b|| with m=n: 3.2850765864159086e-11\n",
      "||Ax-b|| with m>n: 8.074733577929697\n",
      "numpy solution ||Ax - b ||: with m>n: 8.074733579402679\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    n = np.random.randint(2, 5)\n",
    "    A = np.random.rand(n, n)\n",
    "    b = np.random.rand(n)\n",
    "\n",
    "    x1 = least_squares(A, b)\n",
    "    x2 = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "    \n",
    "    # overdetermined\n",
    "    m = np.random.randint(n+1, 10)\n",
    "    A1 = np.random.rand(m, n)\n",
    "    b1 = np.random.rand(m)   \n",
    "    \n",
    "    x1_m = least_squares(A, b)\n",
    "    x2_m = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "\n",
    "    print(f'test no: {i}')\n",
    "    print(f'||Ax-b|| with m=n: {np.linalg.norm(A@x1-b)}')\n",
    "    print(f'||Ax-b|| with m>n: {np.linalg.norm(A1@x1_m-b1)}')\n",
    "    print(f'numpy solution ||Ax - b ||: with m>n: {np.linalg.norm(A1@x2_m-b1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsQLT38gVbn_"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLwlnOzuV-Cd"
   },
   "source": [
    "All of the algorithms passed the given tests within a tolerance of round-off error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4GLBv0zWr7m"
   },
   "source": [
    "# **Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bcsDSoRXHZe"
   },
   "source": [
    "\n",
    "All implemented algorithms both found satisfactory solutions to the given problem and gave the same answers as the default numpy algorithms. For the QR factorization and direct solver only solutions with square matrices were reviewed, which could be further investigated for other shapes. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "template-report-lab-X.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
