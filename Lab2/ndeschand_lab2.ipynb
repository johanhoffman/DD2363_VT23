{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT23/blob/main/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"6RgtXlfYO_i7"},"source":["# **Lab 2: Iterative methods**\n","**Nolwenn Deschand**"]},{"cell_type":"markdown","metadata":{"id":"9x_J5FVuPzbm"},"source":["# **Abstract**"]},{"cell_type":"markdown","metadata":{"id":"6UFTSzW7P8kL"},"source":["\n","In this lab, we will deal with iterative methods for solving linear and nonlinear equations. Iterative methods are ways to deal with the problem of storage of nonzero components, which can make direct methods too expensive for large sparse systems. Iterative methods generate a sequence of approximated solutions, by reducing the error at each iteration. We will implement three different iterative methods: the Jacobi and Gauss-Seidel iterations for solving linear equations of the form $Ax = b$ and the Newton’s method for the nonlinear equation $f(x) = 0$.\n","\n","---\n","\n","Most of the algorithms are implemented from the pseudo-code present in the chapters 7 and 8 of the book *Methods in Computational Science*, from Johan Hoffman."]},{"cell_type":"markdown","metadata":{"id":"OkT8J7uOWpT3"},"source":["#**About the code**"]},{"cell_type":"markdown","metadata":{"id":"HmB2noTr1Oyo"},"source":["A short statement on who is the author of the file, and if the code is distributed under a certain license. "]},{"cell_type":"code","metadata":{"id":"Pdll1Xc9WP0e","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"5e266416-f600-4ea2-a611-5dce718839ca","executionInfo":{"status":"ok","timestamp":1675603026601,"user_tz":-60,"elapsed":14,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}}},"source":["\"\"\"This program is a lab report using the provided template\"\"\"\n","\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n","\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n","\n","# written by Nolwenn Deschand (ddeschand@kth.se)\n","# Template by Johan Hoffman\n"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'KTH Royal Institute of Technology, Stockholm, Sweden.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"28xLGz8JX3Hh"},"source":["# **Set up environment**"]},{"cell_type":"markdown","metadata":{"id":"D2PYNusD08Wa"},"source":["To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "]},{"cell_type":"code","metadata":{"id":"Xw7VlErAX7NS","executionInfo":{"status":"ok","timestamp":1675603030634,"user_tz":-60,"elapsed":237,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}}},"source":["# Load neccessary modules.\n","from google.colab import files\n","\n","import numpy as np\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnO3lhAigLev"},"source":["# **Introduction**"]},{"cell_type":"markdown","metadata":{"id":"l5zMzgPlRAF6"},"source":[" Iterative methods are useful to solve linear and nonlinear equations, for which direct methods can be too expensive. The main difference between direct and iterative methods, is that iterative methods do not have a fixed number of steps. Each step computes an approximation of the solution, which is improved in each step. The algorithm stops when the stopping criterion is satisfied, usually a tolerance that we set.\n","\n","\n","We will start by implement two iterative methods for solving the linear equation $Ax = b$ with A a matrix, x the solution and b a vector. We will use the Jacobi and Gauss-Seidel iterations, two methods than can under some conditions be equivalent to the left-preconditioned Richardson iteration. Instead of solving $Ax = b$, we will use the left precondition $BAx = Bx$, that will improve the rate of convergence. \n","\n","\n","In a second part, we will implement Newton's method for solving nonlinear equations of the form $f(x) = 0$. Three iterative methods are available for this objective: the bisection, the fixed point iteration, and Newton’s method. The bisection methos is robust andd simple but its rate of convergence is slow. The fixed point iteration is a powerful general method that is straightforward to generalize to systems of nonlinear equations. Under some parameters, the fixed point iteration correspond to Newton’s method to enhance the rate of convergence. With Newton's method, we use the the derivative of the function in terms of the tangent line to estimate the new approximation."]},{"cell_type":"markdown","metadata":{"id":"jOQvukXZq5U5"},"source":["# **Method**"]},{"cell_type":"markdown","metadata":{"id":"zF4iBj5VURZx"},"source":["**Implementation of the Jacobi iteration for $Ax = b$**\n","\n","The Jacobi iteration is equivalent to Jacobi left preconditioned Richardson iteration with $α = 1$, that is solving the equation $BAx = Bb$ for a specific B. In the case of Jacobi iteration, we use for B a diagonal matrix composed of the inverse of the diagonal elements of A.\n","\n","To implement this method, we will first implement the Richardson Iteration using the following algorithm from chapter 7: \n","```\n","ALGORITHM 7.1. x = richardson_iteration(A, b, alpha). \n","Input: n x n system matrix A, n vector b, parameter alpha. \n","Output: solution vector x.\n","1: x=0\n","2: while norm(r)/norm(b) > TOL do\n","3:     r = matrix_vector_product(A, x) \n","4:     r[:] = b[:] - r[:]\n","5:     x[:] = x[:] + alpha*r[:]\n","6: end while\n","7: return x\n","```\n","We will then implement the left preconditioned Richardson iteration and the Jacobi iteration based on the two previously implemented functions. \n","\n","The jacobi iteration will take as input a matrix A and vector b, and as output a vector x, the solution."]},{"cell_type":"code","source":["def richardson_iteration(A, b, alpha, tolerance):\n","\n","  n = A.shape[0]\n","  x = np.zeros(n)\n","  r = b\n","  while np.linalg.norm(r)/np.linalg.norm(b) > tolerance:\n","    r = np.dot(A,x)\n","    r = b - r\n","    x = x + alpha * r\n","  return x\n","\n","def left_preconditioned_richardson(A, B, b, alpha, tolerance):\n","  x = richardson_iteration(np.dot(B, A), np.dot(B,b), alpha, tolerance)\n","  return x\n","\n","def jacobi_iteration(A, b, tolerance):\n","  Dinv = np.diag(1 / np.diag(A))\n","  x = left_preconditioned_richardson(A, Dinv, b, 1.0, tolerance)\n","  return x"],"metadata":{"id":"MoGkGkLn9Nfs","executionInfo":{"status":"ok","timestamp":1675603027044,"user_tz":-60,"elapsed":18,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["**Implementation of the Gauss-Seidel iteration for $Ax = b$**\n","\n","The Gauss-Seidel iteration is another method for solving linear equations. It is equivalent to a left preconditioned Ridchardson iteration with $B = L^{-1}$ and $α = 1$ (L is the lower triangular matrix obtained from the martix A by zeroing out all entries above the diagonal). Therefore, we can reuse the previously implemented functions ridchardson iteration and left preconditioned ridchardson. \n","\n","As for the Jacobi iteration, the Gauss-Seidel iteration will take as input a matrix A and vector b, and as output a vector x, the solution.\n"],"metadata":{"id":"2uDJHJD39N7n"}},{"cell_type":"code","source":["def gauss_seidel_iteration(A, b, tolerance):\n","  L = np.tril(A)\n","  Linv = np.linalg.inv(L)\n","  x = left_preconditioned_richardson(A, Linv, b, 1.0, tolerance)\n","  return x"],"metadata":{"id":"BVA85AZJ9OUJ","executionInfo":{"status":"ok","timestamp":1675603027045,"user_tz":-60,"elapsed":18,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**Implementation of Newton's method for scalar nonlinear equation $f(x) = 0$**\n","\n","The implementation of Newton's method is based on the algorithm 8.2 in chapter 8 of the book:\n","```\n","ALGORITHM 8.2. x = newtons_method(f, x0). Input: a function f, an initial guess x0.\n","Output: approximate root x.\n","1: x = x0\n","2: while|f(x)|>TOL do\n","3: df = derivative(f, x)\n","4: x = x - f(x)/df\n","5: endwhile \n","6: return x\n","```\n","The input of the function will be a scalar function f(x) and the output a real number number x, the solution. \n"],"metadata":{"id":"Yi07GaQB9OsF"}},{"cell_type":"code","source":["def derivative(f, x):\n","  h = 10**(-10)\n","  return (f(x + h) - f(x - h)) / (2 * h)\n","\n","def newtons_method(f, x0, tolerance):\n","  x = x0\n","  while abs(f(x)) > tolerance:\n","    df = derivative(f,x)\n","    x = x - f(x)/df\n","  return x\n"],"metadata":{"id":"mCktc01r9PDq","executionInfo":{"status":"ok","timestamp":1675607089157,"user_tz":-60,"elapsed":226,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SsQLT38gVbn_"},"source":["# **Results**"]},{"cell_type":"markdown","source":["**Implementation of the Jacobi iteration for $Ax = b$**\n","\n","We will now test the Jacobi iteration. \n","\n","For the jacobi iteration, the convergence criterion is $∥I − D^{−1}A∥ < 1$, with D the matrix composed with the diagonal elements of A. It means that if the matrix x does not fulfill this convergence criterion, the jacobi iteration will not be able to converge and to produce a solution of the equation. In order to fulfill this convergence criterion, the matrix A has to be diagonally dominant. \n","\n","To test the Jacobi iteration, we will start by generating a random diagonally dominant matrix A. Then, we will test the convergence of the residual $|| Ax-b ||$, and $|| x-y ||$ for a manufactured solution y. The expected result is a value very close to zero in both cases."],"metadata":{"id":"nN2O7Di0xVST"}},{"cell_type":"code","source":["n = np.random.randint(2,10)\n","A = np.random.rand(n,n);\n","\n","while (np.linalg.norm(np.identity(n) - np.dot(np.diag(1 / np.diag(A)),A))) >= 1:\n","    A = A + np.diag(np.random.rand(n))\n","\n","\n","# TEST 1 : convergence of residual || Ax-b ||\n","b = np.random.rand(n)\n","\n","x_jacobi1 = jacobi_iteration(A, b, 10**(-15))\n","residual1 = np.linalg.norm(np.dot(A,x_jacobi1)-b)\n","print(\"residual of ||𝐴𝑥−𝑏|| : \", residual1)\n","\n","\n","# TEST 2 : || x-y || for manufactured solution y \n","x = np.random.rand(n)\n","b = np.dot(A, x)\n","\n","x_jacobi2 = jacobi_iteration(A, b, 10**(-15))\n","residual2 = np.linalg.norm(np.dot(A,x_jacobi2)-b)\n","\n","print(\"residual of ||𝑥−𝑦|| : \", residual2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NYez79-xnZG","executionInfo":{"status":"ok","timestamp":1675606549074,"user_tz":-60,"elapsed":222,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}},"outputId":"a9ea91e6-d6d3-45cc-add1-ad73e6658b85"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["residual of ||𝐴𝑥−𝑏|| :  7.977012308035777e-16\n","residual of ||𝑥−𝑦|| :  1.0185048308013224e-14\n"]}]},{"cell_type":"markdown","source":["The two residual values are very close to zero, as expected. The Jacobi iteration was able to produce a solution for $Ax = b$."],"metadata":{"id":"9dlwQwrG6cdM"}},{"cell_type":"markdown","source":["**Implementation of the Gauss-Seidel iteration for $Ax = b$**\n","\n","As for Jacobi iteration, we will need to fulfill the convergence criterion for the Gauss-Seidel iteration to converge. \n","This time, the convergence criterion is $∥I − L^{−1}A∥ < 1$, with L the lower triangular matrix obtained from the martix A by zeroing out all entries above the diagonal. \n","\n","The same generation of matrix A as the Jacobi iteration is enough to fulfill the convergence criterion.\n","\n","Again, we will test the convergence of the residual $|| Ax-b ||$, and $|| x-y ||$ for a manufactured solution y. The expected result is also a value very close to zero in both cases.\n"],"metadata":{"id":"sVTfH9xlxmvT"}},{"cell_type":"code","source":["n = np.random.randint(2,10)\n","A = np.random.rand(n,n);\n","\n","while A is None or (np.linalg.norm(np.identity(n) - np.dot(np.linalg.inv(np.tril(A)),A))) >= 1:\n","    A = A + np.diag(np.random.rand(n))\n","\n","# TEST 1 : convergence of residual || Ax-b ||\n","b = np.random.rand(n)\n","\n","x_seidel1 = gauss_seidel_iteration(A, b, 10**(-15))\n","residual1 = np.linalg.norm(np.dot(A,x_seidel1)-b)\n","print(\"residual of ||𝐴𝑥−𝑏|| : \", residual1)\n","\n","\n","# TEST 2 : || x-y || for manufactured solution y \n","x = np.random.rand(n)\n","b = np.dot(A, x)\n","\n","x_seidel2 = gauss_seidel_iteration(A, b, 10**(-15))\n","residual2 = np.linalg.norm(np.dot(A,x_seidel2)-b)\n","\n","print(\"residual of ||𝑥−𝑦|| : \", residual2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StVPA-2F5DGW","executionInfo":{"status":"ok","timestamp":1675606552024,"user_tz":-60,"elapsed":240,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}},"outputId":"97933da7-9e78-49f7-b517-e007d19ff01b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["residual of ||𝐴𝑥−𝑏|| :  2.7894008272968645e-16\n","residual of ||𝑥−𝑦|| :  6.661338147750939e-16\n"]}]},{"cell_type":"markdown","source":["The residual values are very close to zero. We can consider that the implementation was successful for these test cases."],"metadata":{"id":"7KN1NtF_7voU"}},{"cell_type":"markdown","source":["**Implementation of Newton's method for scalar nonlinear equation $f(x) = 0$**\n","\n","To test the implementation of Newton's method, we will test convergence of residual $|f(x)|$ and $|x-y|$ for a manufactured solution y. We arnitrary choose a function f and a value x0 for the test.\n","\n","For a correct implementation, we expect the residuals to be close to zero.\n"],"metadata":{"id":"Z9VuVaaX9uPO"}},{"cell_type":"code","source":["# TEST 1 : residual of |𝑓(𝑥)|\n","def f(x):\n","  return x**3+42\n","\n","x = newtons_method(f, 2, 10**(-5))\n","\n","print(\"residual of |𝑓(𝑥)| : \",abs(f(x)))\n","\n","# TEST 2 : residual of |𝑥−𝑦|\n","def f2(x):\n","  return x**4 + x**3 + x**2 - 28\n","# expected solution :\n","y = 2\n","x = newtons_method(f2, 2, 10**(-5))\n","print(\"residual of |𝑥−𝑦| : \",abs(x-y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"837lmb7F9vWh","executionInfo":{"status":"ok","timestamp":1675608415812,"user_tz":-60,"elapsed":264,"user":{"displayName":"Nolwenn Dhd","userId":"10907290081410068555"}},"outputId":"8bd9a7fb-db62-4f9d-caed-23a7c2834d78"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["residual of |𝑓(𝑥)| :  7.2869354994509195e-09\n","residual of |𝑥−𝑦| :  0\n"]}]},{"cell_type":"markdown","source":["The results are close to zero as expected. The implementation seems to work for the simple test cases used here."],"metadata":{"id":"XTy02lPmAY_z"}},{"cell_type":"markdown","metadata":{"id":"_4GLBv0zWr7m"},"source":["# **Discussion**"]},{"cell_type":"markdown","metadata":{"id":"6bcsDSoRXHZe"},"source":["Finally, we have implemented three iterative methods, the Jacobi and Gauss-Seidel iteration to solve linear equations, and Newton's method to solve nonlinear equations. The implementations give the expected results for the simple test cases tried here. \n","\n","The efficiency of the methods is not tested here but it could be interesting to look at the rate of convergence and the number of iterations required for convergence for the different thresholds  (arbitrarily chosen at the moment), in order to adjust them according to the precision and computational time desired."]}]}