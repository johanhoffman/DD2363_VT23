{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 2: Iterative methods**\n",
        "**Martin Börjeson**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "This lab is about implementing various iterative methods, which are procedures of no predetermined length, where each iteration improves the result of the previous one. \n",
        "\n",
        "I wrote methods for performing Jacobi iteration, Gauss-Seidel iteration, and Newton's method for scalar and vector functions.\n",
        "\n",
        "All functions passed my limited test suite, though the computational complexity of my Jacobi and Gauss-Seidel iterations is greater than it has to be, which is a result of me directly writing them as a form of left-preconditioned Richardson iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "The code is written by me, Martin Börjeson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7351e526-aad8-4cb2-e2cb-8962877da75b"
      },
      "source": [
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# Report by Martin Börjeson"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import unittest\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "#Methods from lab1\n",
        "def modified_gram_schmidt_iteration(A: np.ndarray):\n",
        "  dims = A.shape\n",
        "  if(dims[0] != dims[1]):\n",
        "    raise Exception(\"Matrix is not quadratic!\")\n",
        "  if(np.linalg.matrix_rank(A)!=A.shape[0]):\n",
        "    raise Exception(\"Matrix is singular!\")\n",
        "  A = np.copy(A)\n",
        "  n = dims[0]\n",
        "  R = np.zeros(A.shape, dtype = float)\n",
        "  Q = np.zeros(A.shape, dtype = float)\n",
        "  for j in range(n):\n",
        "    v = A[:,j]\n",
        "    for i in range(j):\n",
        "      R[i,j] = np.dot(Q[:,i],v)\n",
        "      v -= R[i,j]*Q[:,i]\n",
        "    R[j,j] = np.linalg.norm(v)\n",
        "    Q[:,j] = v/R[j,j]\n",
        "  return Q,R\n",
        "\n",
        "def backward_substitution(R: np.ndarray, v: np.ndarray) -> np.ndarray:\n",
        "  n = v.shape[0]\n",
        "  x = np.zeros(v.shape, dtype = float)\n",
        "  x[-1] = v[-1]/R[-1,-1]\n",
        "  for i in range(n-2,-1,-1):\n",
        "    sum = 0\n",
        "    for j in range(i+1,n):\n",
        "      sum += R[i,j]*x[j]\n",
        "    x[i] = (v[i]-sum)/R[i,i]\n",
        "  return x\n",
        "\n",
        "def solve(A: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "  A = np.copy(A)\n",
        "  b = np.copy(b)\n",
        "  Q,R = modified_gram_schmidt_iteration(A)\n",
        "  return backward_substitution(R,np.dot(np.transpose(Q),b))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "An iterative method is a procedure which takes an undetermined amount of steps, where each step improves upon the result of the previous one. Some mathemathical expressions are infeasible to directly evaluate, such as finding the inverse of a very large matrix. An approximate solution, however, is often sufficient. \n",
        "\n",
        "In this report, I've implemented an algorithm to perform Jacobi and Gauss-Seidel iterations, as well as Newton's method for scalar and vector functions. \n",
        "\n",
        "A class and a few methods from the Numpy library are used. They are the following:\n",
        "\n",
        "`numpy.ndarray` class\n",
        "\n",
        "`numpy.zeros()`\n",
        "\n",
        "`numpy.array()`\n",
        "\n",
        "`numpy.matmul()`\n",
        "\n",
        "`numpy.dot()`\n",
        "\n",
        "`numpy.diag()`\n",
        "\n",
        "`numpy.linalg.norm()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "## **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "Both Jacobi and Gauss-Seidel iteration are both a kind of left preconditioned Richardson iteration on the form $BAx = Bb$, where $Ax = b$ is the equation that we really are trying to solve, but the error reduction in each step is minimized by multiplying both sides with the preconditional matrix $B \\approx A^{-1}$.\n",
        "\n",
        "In Jacobi iteration, $B$ is the diagonal elemenents of the matrix $A$, and in Gauss-Seidel it is the (lower) triangular elements the matrix $A$.\n",
        "\n",
        "\\\\\n",
        "Newton's method is a special case of fixed point iteration $x^{(k+1)}=x^{(k)}+ \\alpha f(x^{(k)})$ where $\\alpha = -f´(x^{(k)})^{-1}$, which gives the approximation quadratic convergence, which is pretty darn fast. The caveat is that the derivative of the evaluated function $f(x)$ has to be computed at each step, which isn't always easy.\n",
        "\n",
        "Newton's method can also be used to find fixed points or roots in vector functions. In that case, the inverted derivative of $f(x)$ is replaced with the inverted jacobian matrix in the formula.\n",
        "\n",
        "My implementations of Newton's methods take both the function and its derivative/jacobian as input, which offloads the hard problem of finding the derivatives to the user of the algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jacobi iteration"
      ],
      "metadata": {
        "id": "ESV17MJqqH86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input: matrix $A$, vector $b$\n",
        "\n",
        "Output: vector $x$ where $x$ is the approximate solution to the equation $b=Ax$ through Jacobi iteration"
      ],
      "metadata": {
        "id": "HX3uAHaOq2ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyparsing.helpers import And\n",
        "def jacobi_iter(A: np.ndarray, b: np.ndarray, itermax = 100, tol = 10**(-10)) -> np.ndarray:\n",
        "  Dvec = np.diag(A)\n",
        "  A1 = np.diag(Dvec)\n",
        "  A1_inv = np.diag(1/Dvec)\n",
        "  A2 = A - A1\n",
        "  Mj = np.matmul(-A1_inv,A2)\n",
        "  c = np.dot(A1_inv,b)\n",
        "  xprev = np.zeros(len(b))\n",
        "  nr = 1\n",
        "\n",
        "  iter = 0\n",
        "  while iter<itermax and nr > tol:\n",
        "    iter +=1\n",
        "    x = np.dot(Mj,xprev) + c\n",
        "    r = x-xprev\n",
        "    xprev = x\n",
        "    nr = np.linalg.norm(r)\n",
        "  return x"
      ],
      "metadata": {
        "id": "v62zuu5lqVR6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gauss-Seidel iteration"
      ],
      "metadata": {
        "id": "mGeb7LhMqkLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input: matrix $A$, vector $b$\n",
        "\n",
        "Output: vector $x$ where $x$ is the approximate solution to the equation $b=Ax$ through Gauss-Seidel iteration"
      ],
      "metadata": {
        "id": "VUesy_K4AJPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inversion of lower triangular matrix\n",
        "def forward_inversion(L: np.ndarray) -> np.ndarray:\n",
        "  n = L.shape[0]\n",
        "  X = np.identity(n, dtype = float)\n",
        "  for i in range(0,n):\n",
        "    X[i] /= L[i,i]\n",
        "    L[i] /= L[i,i]\n",
        "    for j in range(i+1,n):\n",
        "      X[j] -= X[i]*L[j,i]/L[i,i]\n",
        "      L[j] -= L[i]*L[j,i]/L[i,i]\n",
        "  return X\n",
        "\n",
        "def gauss_seidel_iter(A: np.ndarray, b: np.ndarray, itermax = 100, tol = 10**(-10)) -> np.ndarray:\n",
        "  A1 = np.tril(A)\n",
        "  A2 = A-A1\n",
        "  A1_inv = forward_inversion(A1)\n",
        "  Mgs = np.matmul(-A1_inv,A2)\n",
        "  c = np.dot(A1_inv,b)\n",
        "  xprev = np.zeros(len(b))\n",
        "\n",
        "  nr = 1\n",
        "  iter = 0\n",
        "  while iter<itermax and nr > tol:\n",
        "    iter +=1\n",
        "    x = np.dot(Mgs,xprev) + c\n",
        "    r = x-xprev\n",
        "    xprev = x\n",
        "    nr = np.linalg.norm(r)\n",
        "  return x"
      ],
      "metadata": {
        "id": "oBlnsuC3AIfh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's method for scalar nonlinear equation $f(x) = 0$"
      ],
      "metadata": {
        "id": "odg2nSw1qsmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input: scalar function $f(x)$, derivative $f'(x)$, initial guess $x_0$.\n",
        "\n",
        "Output: real number $x$, where $f(x)\\approx0$"
      ],
      "metadata": {
        "id": "18OArV0hP6he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newtons_method(f, df, x0, maxiter = 100, tol = 10**-10):\n",
        "\n",
        "  x = x0\n",
        "  iter = 0\n",
        "\n",
        "  while(iter < maxiter and np.abs(f(x))>tol):\n",
        "    iter += 1\n",
        "    x -= f(x)/df(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "CVuslqYkQWkp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus assignment: Newton's method for vector nonlinear equation $f(x) = 0$"
      ],
      "metadata": {
        "id": "oTb3HDxKP1N_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input: Vector function $f(x)$, jacobian matrix $j_f$, initial guess vector $x_0$.\n",
        "\n",
        "Output: Vector $x$, where $f(x)\\approx 0$"
      ],
      "metadata": {
        "id": "dC2F6IhXULwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newtons_vector_method(f, jf, x0, maxiter = 100, tol = 10**-10):\n",
        "  x = np.copy(x0)\n",
        "  iter = 0\n",
        "\n",
        "  while(iter < maxiter and np.linalg.norm(f(x))>tol):\n",
        "    iter += 1\n",
        "    x -= solve(jf(x),f(x)) #Ax = b solver from lab1\n",
        "  return x"
      ],
      "metadata": {
        "id": "qzBlqe4IP7I8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "Below are a limited set of tests for the varies methods."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jacobi iteration"
      ],
      "metadata": {
        "id": "HLaz7s9EwlIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Jacobi iteration converges if $||I-B^{-1}A || <1$, where B contains the diagonal elements of A. This is the case if A is a diagonally dominant matrix, i.e. that $|a_{i,i}| \\ge \\Sigma_{j \\ne i}|a_i,j|$ for all $i$.\n",
        "\n",
        "I selected a matrix where that condition holds as a test matrix for the iteration.\n",
        "\n",
        "I then calculate the geometric mean of the convergences at every step, which should give a good estimate of the overall convergence"
      ],
      "metadata": {
        "id": "vDTtSRd-_Oih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Convergence of residual\n",
        "\n"
      ],
      "metadata": {
        "id": "0h223WH3yZ3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTS\n",
        "\n",
        "A = np.array([[1,0.2,0,0,0],[0,2,0.1,0,0],[0.4,0,1,0,0],[0.5,0,0,5,0],[0,0.1,0,0,1]])\n",
        "b = np.array([1,2,3,4,5])\n",
        "x = jacobi_iter(A,b)\n",
        "\n",
        "arr = []\n",
        "for i in range(1,100):\n",
        "  arr.append(jacobi_iter(A,b,i))\n",
        "\n",
        "k = 4\n",
        "i = 0\n",
        "sum = 1\n",
        "while(k<len(arr)-1 and np.linalg.norm(arr[k+1]) != np.linalg.norm(arr[k+2])):\n",
        "  k += 1\n",
        "  i += 1\n",
        "  sum *= np.log(np.linalg.norm(arr[k]-arr[k-1])/np.linalg.norm(arr[k-1]-arr[k-2]))/np.log(np.linalg.norm(arr[k-1]-arr[k-2])/np.linalg.norm(arr[k-2]-arr[k-3]))\n",
        "#Linear convergence\n",
        "print(\"Convergence q ≈\", sum**(1/i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhBbbQQ1wl1i",
        "outputId": "22543c8f-ce9d-41dc-b2a8-437af3c1fd91"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergence q ≈ 1.0000000303404102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gauss-Seidel iteration"
      ],
      "metadata": {
        "id": "X64lXdqFwuWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same condition for converging holds for the Gauss-Seidel iteration, that $||I-B^{-1}A ||$ has to be less than 1, though in this case B is instead the (lower) triangular matrix of A. The same matrix used to test jacobi iteration worked in this case also.\n",
        "\n",
        "The geometric mean of the convergence was used here as well, which was useful as the convergence varied greatly between each step (0.65 to 1.53 roughly)."
      ],
      "metadata": {
        "id": "Ec4Ultz_CmAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Convergence of residual"
      ],
      "metadata": {
        "id": "0HKJ0n2o0ytS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTS\n",
        "\n",
        "A = np.array([[1,0.2,0,0,0],[0,2,0.1,0,0],[0.4,0,1,0,0],[0.5,0,0,5,0],[0,0.1,0,0,1]])\n",
        "b = np.array([1,2,3,4,5])\n",
        "x = gauss_seidel_iter(A,b)\n",
        "\n",
        "arr = []\n",
        "for i in range(1,100):\n",
        "  arr.append(gauss_seidel_iter(A,b,i))\n",
        "\n",
        "k = 2\n",
        "i = 0\n",
        "sum = 1\n",
        "temp = 0\n",
        "while(k<len(arr)-1 and np.linalg.norm(arr[k+1]) != np.linalg.norm(arr[k+2])):\n",
        "  k += 1\n",
        "  i += 1\n",
        "  temp = np.log(np.linalg.norm(arr[k]-arr[k-1])/np.linalg.norm(arr[k-1]-arr[k-2]))/np.log(np.linalg.norm(arr[k-1]-arr[k-2])/np.linalg.norm(arr[k-2]-arr[k-3]))\n",
        "  print(\"q ≈\", temp)\n",
        "  sum *= temp\n",
        "  \n",
        "#Linear convergence\n",
        "print(\"Convergence q ≈\", sum**(1/i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx2jUwGgw2t5",
        "outputId": "c53fd207-d5e6-4851-bd70-be4b00deb360"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q ≈ 1.526309990491204\n",
            "q ≈ 0.6551749030211009\n",
            "q ≈ 1.5263099904813349\n",
            "q ≈ 0.6551749030354376\n",
            "q ≈ 1.5263099924098946\n",
            "q ≈ 0.6551749091371564\n",
            "Convergence q ≈ 1.000000001768021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's method for scalar nonlinear equation $f(x) = 0$"
      ],
      "metadata": {
        "id": "0qR3kLWvw7XK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a function which passes through 0 and which is easy to find the derivative to. $f(x) = x^2-x-5,$ $f'(x) = 2x-1$. The convergence seemed to increase at each step, which is why I chose the overall convergence to be the convergence at the gratest value $k$ where $x^{(k+1)} \\ne x^{(k)}$."
      ],
      "metadata": {
        "id": "wmKCS5rzDd8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Convergence of residual"
      ],
      "metadata": {
        "id": "VO5msw9r1awB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTS\n",
        "def scalar_function(x):\n",
        "  return x**2-x-5\n",
        "\n",
        "def scalar_dfunction(x):\n",
        "  return 2*x-1\n",
        "\n",
        "\n",
        "arr = []\n",
        "for i in range(1,100):\n",
        "  arr.append(newtons_method(scalar_function, scalar_dfunction, 0, i))\n",
        "\n",
        "k = 1\n",
        "i = 0\n",
        "sum = 0\n",
        "while(k<len(arr)-1 and np.abs(arr[k+1]) != np.abs(arr[k+2])):\n",
        "  k += 1\n",
        "sum += np.log(np.abs(arr[k]-arr[k-1])/np.abs(arr[k-1]-arr[k-2]))/np.log(np.abs(arr[k-1]-arr[k-2])/np.abs(arr[k-2]-arr[k-3]))\n",
        "\n",
        "#Quadratic convergence\n",
        "print(\"Convergence q ≈\", sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNV5m_MtxBCR",
        "outputId": "ca30c248-dcb8-4d5d-ca2a-856203982390"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergence q ≈ 1.968993291218997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus assignment: Newton's method for vector nonlinear equation $f(x) = 0$"
      ],
      "metadata": {
        "id": "ShLrJxiHxIGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created the vector function by plotting creating two multivariable scalar functions $f(x,y) = z_1$ and $g(x,y) = z_2$ and plotting them in 3d to ensure that their surfaces intersect somewhere where the output is 0. This means that the vector function $F(x,y) = [f(x,y),g(x,y]^T$ maps some input to the zero-vector.\n",
        "\n",
        "I then tested the convergence in the same way as I tested Newton's method for scalar functions."
      ],
      "metadata": {
        "id": "J9p7VKBmEg2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Convergence of residual"
      ],
      "metadata": {
        "id": "UXZ2hi7D1q1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTS\n",
        "\n",
        "#f([x1,x2]) = [(x1)^2 + cos(x2), -x1 + x2^2-8]\n",
        "def vector_function(x: np.ndarray):\n",
        "  return np.array([x[0]**2 + np.cos(x[1]), -x[0] + x[1]**2-8])\n",
        "\n",
        "def vector_dfunction(x: np.ndarray):\n",
        "  return np.array([[2*x[0],-np.sin(x[1])],[-1,2*x[1]]])\n",
        "\n",
        "x0 = np.array([50,25],dtype = float)\n",
        "arr = []\n",
        "for i in range(1,100):\n",
        "  arr.append(newtons_vector_method(vector_function, vector_dfunction, x0, i))\n",
        "\n",
        "k = 6\n",
        "i = 0\n",
        "sum = 0\n",
        "while(k<len(arr)-1 and np.linalg.norm(arr[k+1]) != np.linalg.norm(arr[k+2])):\n",
        "  k += 1\n",
        "sum += np.log(np.linalg.norm(arr[k]-arr[k-1])/np.linalg.norm(arr[k-1]-arr[k-2]))/np.log(np.linalg.norm(arr[k-1]-arr[k-2])/np.linalg.norm(arr[k-2]-arr[k-3]))\n",
        "\n",
        "#Quadratic convergence\n",
        "print(\"Convergence q ≈\", sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcLapfSyxLri",
        "outputId": "98962aa5-7023-4738-aba7-34d47a67e1e9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convergence q ≈ 1.9951656908053532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The limited test results seem to confirm that the methods work as they should. When I wrote the tests for Newton's method of vector functions, I first got a linear convergence instead of a quadratic one. I later discovered that I had made a mistake when writing the jacobian, which of course messed up the convergence. It's interesting that the method converged despite a wrongly defined jacobian, at least in my test case. It was also interesting that the convergence varied between the iteration steps for the Gauss-Seidel iteration. Not sure why that was the case.\n",
        "\n",
        "My implementations of Jacobi and Gauss-Seidel iteration are both written as a Richardson iteration, which I've now realized might be a mistake, as I use matrix multiplication to create the iteration matrix $M_J$ and $M_{GS}$. The computational complexity of general matrix multiplication is the same as that of matrix inversion, which is exactly what we were trying to get away from. Each iteration step in my implementation has quadratic complexity, as it should, but complexity for the creation of the iteration matrix is higher.\n",
        "\n",
        "If I were to redo my implementations, I would instead write them in terms of the components of the matrix $A$, which has a quadratic computational complexity for each iteration step, and no setup required."
      ]
    }
  ]
}