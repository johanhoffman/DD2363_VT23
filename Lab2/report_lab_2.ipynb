{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OkT8J7uOWpT3",
        "28xLGz8JX3Hh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NogginBops/DD2363_VT23/blob/main/Lab2/report_lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 2: Iterative Methods**\n",
        "**Julius Häger**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ"
      },
      "source": [
        "In this lab I implement two iterative methods for linear systems of equations. Jacobian iteration and Gauss-Seidel iteration. I also implement Newton's method for non-linear scalar equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a46c4f5f-53e6-4612-d7f3-1cf2abd2da3a"
      },
      "source": [
        "\"\"\"This program is lab report in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2023 Julius Häger (juliusha@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import display, Math\n",
        "\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common \n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "    \n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "To solve big systems the computational cost of direct methods for solving these systems becomes large. Iterative methods has potential to be much faster in this regard as they don't need to compute the exact solution, they can stop after some tolerance is reached.\n",
        "\n",
        "Iterative methods often have the advantage that they only need to compute matrix-vector products. This makes it easy to exploit knowledge about the system, e.g. if the matrix is sparse it's easy to implement a sparse matrix-vector product.\n",
        "\n",
        "The simplest method for iteratively solving linear systems of equations is Richardson iteration where the next value $\\mathbf{x}^{(k+1)}$ is given by $\\mathbf{x}^{(k+1)} = (I - \\alpha A)\\mathbf{x}^{(k)} + \\mathbf{b}$, where $\\alpha > 0$ is an arbitrary parameter of the method.\n",
        "\n",
        "The convergance of Richardson iteration is given by $|| I - \\alpha A ||$ so to get fast convergence we want $|| I - \\alpha A||$ to be close to a zero matrix. This doesn't always happen, but what we can do is predcondition the system $Ax = b$ with some other matrix $B$ to get a new system of equations with the same solution as the first solution $BAx = Bb$. This gives us the new iteration $\\mathbf{x}^{(k+1)} = (I - \\alpha BA)\\mathbf{x}^{(k)} + \\alpha Bb$. To find a good predondition matrix we want to minimize $|| I - \\alpha BA ||$ by finding a good matrix $B$.\n",
        "\n",
        "The first method implemented for this is jacobi iteration. In jacobi iteration we use $B = \\mathrm{diag}(A)^{-1}=D{-1}$ and set $\\alpha = 1$. This gives us the iteration $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + D^{-1}(\\mathbf{b} - A\\mathbf{x}^{(k) })$ where multiplication by $D^{-1}$ can be implemented as element-wise division by the diagonal of $D$.\n",
        "\n",
        "The second method implemented is called Gauss-Seidel iteration. Here the predondition matrix is $B = \\mathrm{tril}(A)^{-1} = L^{-1}$ and $\\alpha = 1$ which gives the iteration $L\\mathbf{x}^{(k+1)} = \\mathbf{b} - (A - L)\\mathbf{x}^{(k)}$, from which $\\mathbf{x}^{(k+1)}$ can be solved for with forward-substitution.\n",
        "\n",
        "For non-linear scalar equations, solutions can be found using Newtons method. This method utilizes the derivative of the continous function to calculate the intersection of the tangent with the x-axis, which is then used for the next iteration. A version of this called the secant method doesn't require an explicit derivative, and instead calculates a secant from two initial guesses and uses the intersection with the x-axis as one of it's two gueses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 1: Jacobi iteration for $Ax=b$"
      ],
      "metadata": {
        "id": "lgT1y4-b9a2p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "To implement jacobi iteration we can take two routes. The first route is to implement a richardson iteration procedure and then pass it jacobi iteration preconditioned matrices; this is implemented in the `jacobi_iteration(A, b, TOL=1e-16)` procedure.\n",
        "The other route is to utilize the knowledge that multiplication by $D^{-1}$ is equivalent to division of the diagonal elements of $D$. This version is implemented as `jacobi_iteration_direct(A, b, TOL=1e-16)`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment 1: Jacobi iteration for Ax=b\n",
        "\n",
        "def inverse_diag(A : np.ndarray):\n",
        "  return np.diag(np.reciprocal(np.diag(A)));\n",
        "\n",
        "def richardson_iteration(A : np.ndarray, b : np.ndarray, alpha, TOL = 1e-16):\n",
        "  x = np.zeros_like(b)\n",
        "  r = b.copy()\n",
        "  #display(Math(rf\"|| A || = {np.linalg.norm(A, ord=2)}\"))\n",
        "  iteration = 0\n",
        "  while np.linalg.norm(r) / np.linalg.norm(b) > TOL:\n",
        "    #print(f\"x{iteration} = {x}\", \"r\", r, \"x\", x, \"tol\", np.linalg.norm(r) / np.linalg.norm(b))\n",
        "    r = b - (A @ x)\n",
        "    x = x + alpha * r\n",
        "    iteration += 1\n",
        "\n",
        "  #print(f\"iterations = {iteration}\")\n",
        "  return x\n",
        "\n",
        "def jacobi_iteration(A : np.ndarray, b : np.ndarray, TOL = 1e-16):\n",
        "  Di = inverse_diag(A)\n",
        "  M = Di @ A\n",
        "  c = Di @ b\n",
        "  #display(Math(rf\"|| I - D^{{-1}}A || = {np.linalg.norm(M, ord=2)}\"))\n",
        "  return richardson_iteration(M, c, 1, TOL)\n",
        "\n",
        "def jacobi_iteration_direct(A : np.ndarray, b : np.ndarray, TOL = 1e-16):\n",
        "  x = np.zeros(b.size)\n",
        "  r = b.copy()\n",
        "  D = np.diag(A)\n",
        "  \n",
        "  R = A - np.diag(np.diag(A))\n",
        "\n",
        "  Di = np.reciprocal(D)\n",
        "\n",
        "  MAX_ITERATIONS = 10_0000\n",
        "  iterations = 0\n",
        "  while (np.linalg.norm(r) / np.linalg.norm(b)) > TOL and iterations < MAX_ITERATIONS:\n",
        "    r = x.copy()\n",
        "    x = (b - (R @ x)) * Di\n",
        "    r = r - x\n",
        "\n",
        "    iterations += 1\n",
        "\n",
        "  #print(np.linalg.norm(r) / np.linalg.norm(b))\n",
        "  #print(\"iterations:\", iterations)\n",
        "  return x"
      ],
      "metadata": {
        "id": "Es-YHKleGfzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Gauss-Seidel iteration for $Ax=b$"
      ],
      "metadata": {
        "id": "_Am8e3q_9ETG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement Gauss-Seidel iteration we need to implement the iteration $L\\mathbf{x}^{(k+1)} = b - (A - L)\\mathbf{x}^{k}$. This can be done by rewriting the equation in terms of the forward substitusion done to solve the equation. The resulting form is $\\mathbf{x}^{(k+1)} = D^{-1}(b - L\\mathbf{x}^{(k+1)} - U\\mathbf{x}^{(k)})$ with $U$ being the strictly upper triangular part of $A$, $L$ being the strictly lower triangular part of $A$, and $D$ being the diagonal part of $A$.\n",
        "\n",
        "This can very simply be implemented by keeping one vector $\\mathbf{x}$ that is partially updated so that $\\mathbf{x}_{j} = \\mathbf{x}_i^{(k+1)}$ for all $j < i$ and $\\mathbf{x}_{j} = \\mathbf{x}_i^{k}$ for all $j > i$. This is implemented as `gauss_seidel_direct(A, b, TOL=1e-16)`\n",
        "\n",
        "Another way to solve this is to calculate $L^{-1}$ and left predcondition the system with that matrix and do richardson iteration on that system. This is implemented as `gauss_seidel(A, b, TOL=1e-16)`."
      ],
      "metadata": {
        "id": "bIMbknLW9Y0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment 2: Gauss-Seidel iteration for Ax=b\n",
        "def gauss_seidel(A : np.ndarray, b : np.ndarray, TOL = 1e-16):\n",
        "  Li = np.linalg.inv(np.tril(A))\n",
        "  #print(\"Li\", Li)\n",
        "  #print(\"b\", b)\n",
        "  M = Li @ A\n",
        "  c = Li @ b\n",
        "  #print(\"M\", M)\n",
        "  #print(\"c\", c)\n",
        "  return richardson_iteration(M, c, 1, TOL)\n",
        "\n",
        "def gauss_seidel_direct(A : np.ndarray, b : np.ndarray, TOL = 1e-16):\n",
        "  n, m = A.shape\n",
        "  x = np.zeros_like(b)\n",
        "  r = b.copy()\n",
        "  D = np.diag(np.diag(A))\n",
        "  AD = A - D\n",
        "  iterations = 0\n",
        "  while np.linalg.norm(r) / np.linalg.norm(b) > TOL:\n",
        "    r = x.copy()\n",
        "    for i in range(n):\n",
        "      sigma = AD[i, :] @ x\n",
        "      x[i] = (b[i] - sigma) / A[i, i]\n",
        "    r = r - x\n",
        "    iterations += 1\n",
        "  #print(f\"Iterations: {iterations}\")\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "1dHJqK9ZNHGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UrjkusxI9iYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 3: Newton's method for scalar nonlinear equation $f(x)=0$"
      ],
      "metadata": {
        "id": "QXLC7sBkLaI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I have implemented two variations of Newtons method, one called `newton_proper(f, dfdx, x0)` that takes a function `f(x)` and it's derivative `dfdx(x)` and solves $f(x) = 0$ with an initial guess $x_0$. The second implementation is the secant method `secant_method(f, x0, x1)` which similarly takes in the function `f(x)` as well as two initial guesses $x_0$ and $x_1$."
      ],
      "metadata": {
        "id": "tg9CuXtNDciE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment 3: Newton's methods for scalar nonlinear equation f(x) = 0\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "def newton_proper(f : Callable[[np.double], np.double], dfdx : Callable[[np.double], np.double], x0, TOL = 1e-30):\n",
        "  x = x0\n",
        "  while np.abs(f(x)) > TOL:\n",
        "    x = x - (f(x) / dfdx(x))\n",
        "  return x\n",
        "\n",
        "def secant_method(f : Callable[[np.double], np.double], x0, x1, TOL = 1e-30):\n",
        "  while np.abs(f(x1)) > TOL:\n",
        "    (x0, x1) = (x1, x1 - f(x1) * ((x1 - x0) / (f(x1) - f(x0))))\n",
        "  return x1"
      ],
      "metadata": {
        "id": "iEjFDHqxpKuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 1: Jacobi iteration for $Ax=b$"
      ],
      "metadata": {
        "id": "KrXRuUqK1Wbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify that the solution obtained from the jacobi iteration is correct we can check that $|| Ax - b ||$ is equal or very close to zero. We can also compare the accuired solution $x$ to a calculated exact solution $y$, $|| x - y ||$."
      ],
      "metadata": {
        "id": "gwcQJQ3f7wzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 1. Tests\n",
        "\n",
        "A = np.array([[2, 1, 0], [0, 2, 1], [1, 0, 3]], dtype=np.double)\n",
        "b = np.array([2, 1, 4], dtype=np.double)\n",
        "y = np.array([1.0, 0, 1])\n",
        "\n",
        "solution = jacobi_iteration(A, b)\n",
        "solution2 = jacobi_iteration_direct(A, b)\n",
        "\n",
        "display(rf\"Jacobi iteration\")\n",
        "display(Math(rf\"x = \\begin{{bmatrix}} {solution[0]} \\\\ {solution[1]} \\\\ {solution[2]} \\end{{bmatrix}}\"))\n",
        "display(Math(rf\"|| Ax - b || = {np.linalg.norm((A @ solution) - b)}\"))\n",
        "display(Math(rf\"|| x - y || = {np.linalg.norm(solution - y)}\"))\n",
        "\n",
        "display(rf\"Jacobi iteration direct\")\n",
        "display(Math(rf\"x = \\begin{{bmatrix}} {solution2[0]} \\\\ {solution2[1]} \\\\ {solution2[2]} \\end{{bmatrix}}\"))\n",
        "display(Math(rf\"|| Ax - b || = {np.linalg.norm((A @ solution2) - b)}\"))\n",
        "display(Math(rf\"|| x - y || = {np.linalg.norm(solution2 - y)}\"))\n"
      ],
      "metadata": {
        "id": "t2C63BH8C_sh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "c299d051-bf76-4c2d-c1cd-372e56691791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Jacobi iteration'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle x = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || Ax - b || = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || x - y || = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Jacobi iteration direct'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle x = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || Ax - b || = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || x - y || = 0.0$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And as we can see the result of both implementations converge. And in this case has zero representable error."
      ],
      "metadata": {
        "id": "MoyWev7m1f55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2: Gauss-Seidel iteration for $Ax=b$"
      ],
      "metadata": {
        "id": "aVM3sYKA1krK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify this the Gauss-Seidel iteration we do the same tests as for the Jacobi iteration. We compare $Ax$ to $b$ to see how similar they are using $|| Ax - b ||$, and we compare the solution $x$ to a calculated solution $y$ with $|| x - y ||$."
      ],
      "metadata": {
        "id": "yHRXj-gqBgJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 2. Tests\n",
        "\n",
        "solution = gauss_seidel(A, b)\n",
        "solution2 = gauss_seidel_direct(A, b, 1e-17)\n",
        "\n",
        "display(rf\"Gauss-Seidel iteration\")\n",
        "display(Math(rf\"x = \\begin{{bmatrix}} {solution[0]} \\\\ {solution[1]} \\\\ {solution[2]} \\end{{bmatrix}}\"))\n",
        "display(Math(rf\"|| Ax - b || = {np.linalg.norm((A @ solution) - b)}\"))\n",
        "display(Math(rf\"|| x - y || = {np.linalg.norm(solution - y)}\"))\n",
        "\n",
        "display(rf\"Gauss-Sidel iteration direct\")\n",
        "display(Math(rf\"x = \\begin{{bmatrix}} {solution2[0]} \\\\ {solution2[1]} \\\\ {solution2[2]} \\end{{bmatrix}}\"))\n",
        "display(Math(rf\"|| Ax - b || = {np.linalg.norm((A @ solution2) - b)}\"))\n",
        "display(Math(rf\"|| x - y || = {np.linalg.norm(solution2 - y)}\"))\n"
      ],
      "metadata": {
        "id": "AFzBCIxIuXhI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "c7ad953e-2168-4988-e5c4-2d2a5e5c7499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Li [[ 0.5         0.          0.        ]\n",
            " [ 0.          0.5         0.        ]\n",
            " [-0.16666667  0.          0.33333333]]\n",
            "b [2. 1. 4.]\n",
            "M [[ 1.          0.5         0.        ]\n",
            " [ 0.          1.          0.5       ]\n",
            " [ 0.         -0.16666667  1.        ]]\n",
            "c [1.  0.5 1. ]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Gauss-Seidel iteration'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle x = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || Ax - b || = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || x - y || = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Gauss-Sidel iteration direct'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle x = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || Ax - b || = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle || x - y || = 0.0$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two implementations converge and the error is not representable."
      ],
      "metadata": {
        "id": "DxPOe4o31vOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 3: Newton's method for scalar nonlinear equation $f(x)=0$"
      ],
      "metadata": {
        "id": "vLnuQBcg2IZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify that the non-linear scalar equation solvers work we can calculate $|f(x)|$ and see if this is indeed zero (or very close to zero). We can also compare this to a calculated answer, in this case the solution to the equation $x^2 - 17 = 0$ is $x = \\sqrt{17}$."
      ],
      "metadata": {
        "id": "dI0XKb_NF80d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assignment 3. Tests\n",
        "\n",
        "f = lambda x : x * x - 17\n",
        "dfdx = lambda x : 2 * x\n",
        "\n",
        "solution = newton_proper(f, dfdx, 2)\n",
        "\n",
        "solution2 = secant_method(f, -1, 10)\n",
        "\n",
        "y = np.sqrt(17)\n",
        "\n",
        "display(\"Newtons method\")\n",
        "display(Math(rf\"|f(x)| = {np.abs(f(solution))}\"))\n",
        "display(Math(rf\"| x-y | = {np.abs(solution - y)}\"))\n",
        "\n",
        "display(\"Secant method\")\n",
        "display(Math(rf\"|f(x)| = {np.abs(f(solution2))}\"))\n",
        "display(Math(rf\"| x-y | = {np.abs(solution2 - y)}\"))"
      ],
      "metadata": {
        "id": "MDAx4ULDulwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "f15a4f7f-3f11-4e97-cb1d-572559ba92e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Newtons method'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle |f(x)| = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle | x-y | = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Secant method'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle |f(x)| = 0.0$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle | x-y | = 0.0$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, both newtons method and the secant method converge on the correct result."
      ],
      "metadata": {
        "id": "HcQMyS6O2hQR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "When implementing these kinds of procedures you often way to put them through their paces by running more tests with more edge cases. For example the iterative methods could check for divergence and report that is some way. They should also be tested on more matrices to guarantee their correctness. This was something I had great trouble getting right with both of the Gauss-Seidel implementation, so there might still exist some issues within these procedures."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the newtons method implementation I decided not to use something like automatic differentiation or symbolic differentiation as their implementation seemed quite complicated. Insated I opted for implementing one version with explicit drivatives (`newton_proper`) and one version which doesn't require explicit derviatives (`secant_method`)."
      ],
      "metadata": {
        "id": "t_-uC1rr7uKf"
      }
    }
  ]
}