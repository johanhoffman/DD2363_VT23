{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT23/blob/main/Lab2/reinisfreibergs_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RgtXlfYO_i7"
   },
   "source": [
    "# Lab 1: iterative methods \n",
    "**Reinis Freibergs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x_J5FVuPzbm"
   },
   "source": [
    "# **Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UFTSzW7P8kL"
   },
   "source": [
    "\n",
    "The objective of the lab is to implement and test various iterative methods - the Jacobi iteration, Gauss-Seidel iteration, Newton's method for scalar nonlinear equation and Newton's method for vector nonlinear equations.\n",
    "\n",
    "All algorithms were tested with random matrices with the assumed configuration and returned the expected outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkT8J7uOWpT3"
   },
   "source": [
    "# **About the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "Pdll1Xc9WP0e",
    "outputId": "9e782dc7-4ad0-48f9-d45c-2200e83dec4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
    "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
    "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
    "\n",
    "\n",
    "# Author: Reinis Freibergs, 2023\n",
    "\n",
    "# Based on a template:\n",
    "# Copyright (C) 2023 Johan Hoffman (jhoffman@kth.se)\n",
    "\n",
    "\n",
    "# This file is part of the course DD2363 Methods in Scientific Computing\n",
    "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
    "#\n",
    "# This is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Lesser General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This template is maintained by Johan Hoffman\n",
    "# Please report problems to jhoffman@kth.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28xLGz8JX3Hh"
   },
   "source": [
    "# **Set up environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2PYNusD08Wa"
   },
   "source": [
    "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "Xw7VlErAX7NS"
   },
   "outputs": [],
   "source": [
    "# Load neccessary modules.\n",
    "#from google.colab import files\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnO3lhAigLev"
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5zMzgPlRAF6"
   },
   "source": [
    "In this lab iterative methods for solving systems of linear and nonlinear equations are examined. In comparison to direct methods based on matrix factorization, iterative methods generate approximate solutions, as a result being much faster and having lower memory demands, which is especially important for very large systems. Of course, approximation also requires to follow the errors and convergence rates.\n",
    "\n",
    "\n",
    "In this report the implementations of iterative methods for finding the solutions of systems of linear and nonlinear equations are given:<br>\n",
    "1.  Jacobi iteration\n",
    "<br>\n",
    "2. Gauss - Seidel iteration\n",
    "<br>\n",
    "3. Newton's method for scalar nonlinear equation\n",
    "4. Newton's method for vector nonlinear equation\n",
    "\n",
    "All implementations are based on materials from the lecture notes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOQvukXZq5U5"
   },
   "source": [
    "# **Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobi iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stationary iterative methods are based on the fixed point iteration ($\\textit{eq. 7.10}$) to solve the equation $x=g(x)$:\n",
    "\n",
    "$$x^{k+1} = g(x^k)$$\n",
    "\n",
    "by putting the linear operator $g(x) = Mx^{k} + c$. Then $M \\in R^{nxn}$ is the iteration matrix, ${x}^k$ is the sequence of approximations and $x^k \\in R^n$ together with $c \\in R^n$ are constant vectors.\n",
    "\n",
    "Chapter 7.7 in the book describes matrix splitting - a method to formulate a stationary iterative method by splitting the matrix $A$ into a sum of two matrices:\n",
    "\n",
    "$$A = A_1 + A_2$$\n",
    "\n",
    "where $A_1$ is an easily invertible matrix. The Jacobi iteration described in example $\\textit{7.8}$ is based on the splitting:\n",
    "\n",
    "$$A_1 = D,     A_2 = A - D$$\n",
    "\n",
    "where $D = diag(A)$. In that case the iteration matrix $M$ is given by:\n",
    "\n",
    "$$M = -D^{-1}(A - D) = (I - D^{-1}A)$$\n",
    "\n",
    "and the vector $c$:\n",
    "\n",
    "$$c = D^{-1}b$$\n",
    "\n",
    "Finally giving the form:\n",
    "\n",
    "$$x^{(k+1)} = -D^{-1}(A - D)x^{(k)} + D^{-1}b$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence criterion is given by:\n",
    "\n",
    "$$\\Vert I - D^{-1}A \\Vert \\le 1$$ \n",
    "\n",
    "Meaning that the matrix $A$ must be diagonally dominant for the solution to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_iteration(A, b, tol):\n",
    "    \n",
    "    d = A * np.eye(A.shape[0])\n",
    "    a_2 = A - d\n",
    "    # since its diagonal d^-1[i,i] = 1/d[i,i]\n",
    "    d_inv = np.divide(1, d, out=np.zeros_like(d), where=d != 0)\n",
    "    x = np.zeros_like(b)\n",
    "  \n",
    "    iterations = 0\n",
    "    while np.linalg.norm(A@x - b) / np.linalg.norm(b) > tol and iterations < 10:\n",
    "        x = d_inv @ (-a_2 @ x + b)\n",
    "        iterations += 1\n",
    "\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Seidel iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gauss-Seidel is similar to the Jacobi iteration, except that the matrix split is done by:\n",
    "\n",
    "$$A_1 = L,     A_2 = A - L$$\n",
    "\n",
    "where L is the lower triangular matrix produced by zeroing out all elements above the diagonal of matrix A, which can be inverted by forward substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_substitution(U, b):\n",
    "    n = len(b)\n",
    "    x = np.zeros(shape=(n,))\n",
    "    x[0] = b[0] / U[0, 0]\n",
    "    for i in range(1, n):\n",
    "        sum = 0\n",
    "        for j in range(0, i):\n",
    "            sum += U[i, j]*x[j]\n",
    "        x[i] = (b[i] - sum)/U[i, i]\n",
    "\n",
    "    return x\n",
    "\n",
    "def gauss_seidel_iteration(A, b, tol):\n",
    "    \n",
    "    l = np.tril(A)\n",
    "    a_2 = A - l\n",
    "\n",
    "    x = np.zeros_like(b)\n",
    "     \n",
    "    iterations = 0\n",
    "    while np.linalg.norm(A@x - b) / np.linalg.norm(b) > tol and iterations < 10:\n",
    "        x = forward_substitution(l, (b - a_2 @ x))\n",
    "        iterations += 1\n",
    "\n",
    "    return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method for scalar nonlinear equation $f(x)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the previously viewed Jacobi and Gauss-Seidel iteration, Newton's method works with nonlinear equations. \n",
    "It is defined as the solution of:\n",
    "\n",
    "$$f(x) = 0$$\n",
    "\n",
    "with the fixed point iteration (8.2):\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} + \\alpha f(x^{(k)})$$\n",
    "\n",
    "with $\\alpha = -f'(x^{(k)})^{-1}$, thus giving the final form:\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})} $$\n",
    "\n",
    "which geometrically corresponds to finding the root from the tangent-line.\n",
    "\n",
    "To calculate the derivative $f'(x^{(k)})$ here the central-difference scheme will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(f, x):\n",
    "    delta = 0.001\n",
    "    return (f(x + delta) - f(x - delta)) / (2*delta)\n",
    "\n",
    "def newtons_method(f, x0):\n",
    "    x = x0\n",
    "    tol = 0.001\n",
    "    iterations = 0\n",
    "    while np.linalg.norm(f(x)) > tol and iterations < 100:\n",
    "        df = derivative(f, x)\n",
    "        x = x - f(x)/df\n",
    "        iterations += 1\n",
    "        \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method for vector nonlinear equation $f(x)=0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method for a system of nonlinear equations is analogous to the previously implemented case of the scalar equation, but instead there is a system of equations and instead of the derative a Jacobian matrix is used, which includes all partial derivatives:\n",
    "\n",
    "$$\n",
    "D = \\begin{pmatrix}\n",
    "  \\frac{\\partial u_1}{\\partial x_1} & \n",
    "    \\dots & \n",
    "    \\frac{\\partial u_1}{\\partial x_n} \\\\[1ex] % <-- 1ex more space between rows of matrix\n",
    "  \\vdots & \n",
    "    \\vdots & \n",
    "    \\vdots \\\\[1ex]\n",
    "  \\frac{\\partial u_n}{\\partial x_1} & \n",
    "    \\dots & \n",
    "    \\frac{\\partial u_n}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_vector(f, x):\n",
    "    tol = 0.001\n",
    "    iterations = 0\n",
    "    while np.linalg.norm(f(x)) > tol and iterations < 100:\n",
    "        j = jacobian(x)\n",
    "        dx = np.linalg.solve(j, -f(x))\n",
    "        x = x + dx\n",
    "        iterations += 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsQLT38gVbn_"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobi iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobi iteration is tested for random matrices. As the method can converge only for diagonally dominant matrices, we start buy testing completely diagonal matrices and move to subsequently less diagonal-dominant by introducing random elements, thus testing the limits of the method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated residuals are $\\|Ax - b \\|$ and $\\|x-y \\|$ where y is a manufactured solution with $b=Ay$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-zero probability: 0\n",
      "matrix:\n",
      "[[0.63 0.   0.   0.   0.  ]\n",
      " [0.   0.65 0.   0.   0.  ]\n",
      " [0.   0.   0.31 0.   0.  ]\n",
      " [0.   0.   0.   0.76 0.  ]\n",
      " [0.   0.   0.   0.   0.95]]\n",
      "\n",
      "||Ax - b||: 7.97218214573518e-17\n",
      "||x - y||: 0.0\n",
      "\n",
      "non-zero probability: 0.05\n",
      "matrix:\n",
      "[[0.63 0.   0.99 0.   0.  ]\n",
      " [0.   0.65 0.   0.   0.16]\n",
      " [0.   0.   0.31 0.   0.  ]\n",
      " [0.   0.   0.   0.76 0.  ]\n",
      " [0.   0.   0.   0.   0.95]]\n",
      "\n",
      "||Ax - b||: 2.2929868617541516e-16\n",
      "||x - y||: 1.1102230246251565e-16\n",
      "\n",
      "non-zero probability: 0.1\n",
      "matrix:\n",
      "[[0.63 0.   0.   0.   0.  ]\n",
      " [0.   0.65 0.   0.   0.  ]\n",
      " [0.   0.   0.31 0.   0.  ]\n",
      " [0.   0.   0.   0.76 0.  ]\n",
      " [0.   0.   0.   0.   0.95]]\n",
      "\n",
      "||Ax - b||: 7.97218214573518e-17\n",
      "||x - y||: 0.0\n",
      "\n",
      "non-zero probability: 0.15\n",
      "matrix:\n",
      "[[0.63 0.87 0.   0.   0.  ]\n",
      " [0.   0.65 0.   0.   0.  ]\n",
      " [0.   0.   0.31 0.   0.  ]\n",
      " [0.   0.   0.   0.76 0.  ]\n",
      " [0.   0.   0.   0.   0.95]]\n",
      "\n",
      "||Ax - b||: 5.721958498152797e-17\n",
      "||x - y||: 6.206335383118183e-17\n",
      "\n",
      "non-zero probability: 0.2\n",
      "matrix:\n",
      "[[0.63 0.   0.   0.   0.  ]\n",
      " [0.   0.65 0.96 0.   0.  ]\n",
      " [0.   0.   0.62 0.   0.  ]\n",
      " [0.61 0.74 0.   0.76 0.  ]\n",
      " [0.   0.   0.32 0.   0.95]]\n",
      "\n",
      "||Ax - b||: 5.551115123125783e-17\n",
      "||x - y||: 1.0007415106216802e-16\n",
      "\n",
      "non-zero probability: 0.25\n",
      "matrix:\n",
      "[[0.63 0.   0.   0.44 0.59]\n",
      " [0.   0.65 0.   0.   0.  ]\n",
      " [0.   0.77 0.31 0.32 0.  ]\n",
      " [0.   0.   0.   1.53 0.  ]\n",
      " [0.   0.59 0.   0.   0.95]]\n",
      "\n",
      "||Ax - b||: 5.551115123125783e-17\n",
      "||x - y||: 2.5296051550047473e-16\n",
      "\n",
      "non-zero probability: 0.3\n",
      "matrix:\n",
      "[[0.63 0.   0.   0.   0.  ]\n",
      " [0.   1.3  0.   0.   0.  ]\n",
      " [0.   0.77 0.31 0.   0.  ]\n",
      " [0.   0.   0.   0.76 0.  ]\n",
      " [0.   0.59 0.32 0.   0.95]]\n",
      "\n",
      "||Ax - b||: 1.3597399555105182e-16\n",
      "||x - y||: 1.2719202621569003e-16\n",
      "\n",
      "non-zero probability: 0.5\n",
      "matrix:\n",
      "[[0.63 0.87 0.   0.44 0.59]\n",
      " [0.   0.65 0.96 0.24 0.  ]\n",
      " [0.21 0.   0.62 0.32 0.  ]\n",
      " [0.61 0.74 0.   0.76 0.24]\n",
      " [0.   0.59 0.32 0.   1.9 ]]\n",
      "\n",
      "||Ax - b||: 160.77616455684162\n",
      "||x - y||: 231.56120108976137\n",
      "\n",
      "non-zero probability: 1\n",
      "matrix:\n",
      "[[1.26 0.87 0.99 0.44 0.59]\n",
      " [0.7  1.3  0.96 0.24 0.16]\n",
      " [0.21 0.77 0.62 0.32 1.  ]\n",
      " [0.61 0.74 0.2  1.53 0.24]\n",
      " [0.88 0.59 0.32 0.15 1.9 ]]\n",
      "\n",
      "||Ax - b||: 957.0354513296691\n",
      "||x - y||: 1313.4400964394215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "A_orig = np.random.rand(n, n)\n",
    "b = np.random.rand(n)\n",
    "for nonzero_element_odds in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 1]:\n",
    "\n",
    "    # all diagonal elemenents and random non-diagonal ones proportional to the probability\n",
    "    A = A_orig * (np.random.rand(n, n) < nonzero_element_odds) + np.diag(np.diag(A_orig))\n",
    "    x = jacobi_iteration(A, b, tol=0.001)\n",
    "    residual_1 = np.linalg.norm(A@x - b)\n",
    "\n",
    "    y = np.random.rand(n)\n",
    "    b_y = A @ y\n",
    "    x_y = jacobi_iteration(A, b_y, tol=0.001)\n",
    "    residual_2 = np.linalg.norm(x_y - y)\n",
    "    \n",
    "    print(f'non-zero probability: {nonzero_element_odds}')\n",
    "    print(f'matrix:')\n",
    "    print(f'{np.round(A,2)}' + '\\n') \n",
    "    print(f'||Ax - b||: {residual_1}')\n",
    "    print(f'||x - y||: {residual_2}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss - Seidel iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing is done identically as the Jacobi iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-zero probability: 0\n",
      "matrix:\n",
      "[[0.87 0.   0.   0.   0.  ]\n",
      " [0.   0.48 0.   0.   0.  ]\n",
      " [0.   0.   0.89 0.   0.  ]\n",
      " [0.   0.   0.   0.04 0.  ]\n",
      " [0.   0.   0.   0.   0.77]]\n",
      "\n",
      "||Ax - b||: 5.551115123125783e-17\n",
      "||x - y||: 0.0\n",
      "\n",
      "non-zero probability: 0.05\n",
      "matrix:\n",
      "[[0.87 0.   0.   0.   0.  ]\n",
      " [0.   0.48 0.   0.   0.  ]\n",
      " [0.   0.   0.89 0.   0.  ]\n",
      " [0.   0.   0.   0.04 0.  ]\n",
      " [0.   0.   0.   0.   1.55]]\n",
      "\n",
      "||Ax - b||: 5.551115123125783e-17\n",
      "||x - y||: 0.0\n",
      "\n",
      "non-zero probability: 0.1\n",
      "matrix:\n",
      "[[0.87 0.   0.   0.85 0.93]\n",
      " [0.   0.48 0.   0.   0.  ]\n",
      " [0.   0.   1.79 0.   0.  ]\n",
      " [0.   0.   0.   0.04 0.  ]\n",
      " [0.   0.64 0.   0.   0.77]]\n",
      "\n",
      "||Ax - b||: 1.2412670766236366e-16\n",
      "||x - y||: 5.551115123125783e-17\n",
      "\n",
      "non-zero probability: 0.15\n",
      "matrix:\n",
      "[[0.87 0.   0.   0.   0.  ]\n",
      " [0.28 0.48 0.   0.   0.  ]\n",
      " [0.   0.   0.89 0.   0.  ]\n",
      " [0.   0.   0.   0.04 0.  ]\n",
      " [0.   0.   0.87 0.91 0.77]]\n",
      "\n",
      "||Ax - b||: 1.1102230246251565e-16\n",
      "||x - y||: 1.5142854222490728e-16\n",
      "\n",
      "non-zero probability: 0.2\n",
      "matrix:\n",
      "[[0.87 0.   0.   0.   0.93]\n",
      " [0.   0.48 0.   0.   0.  ]\n",
      " [0.   0.   0.89 0.   0.  ]\n",
      " [0.8  0.   0.   0.04 0.  ]\n",
      " [0.   0.   0.   0.91 0.77]]\n",
      "\n",
      "||Ax - b||: 233424564762749.12\n",
      "||x - y||: 287181920970055.2\n",
      "\n",
      "non-zero probability: 0.25\n",
      "matrix:\n",
      "[[0.87 0.   0.18 0.85 0.  ]\n",
      " [0.   0.48 0.   0.79 0.  ]\n",
      " [0.   0.53 0.89 0.   0.31]\n",
      " [0.   0.   0.   0.07 0.92]\n",
      " [0.   0.   0.87 0.   1.55]]\n",
      "\n",
      "||Ax - b||: 4042.7577969641475\n",
      "||x - y||: 54171.19742338994\n",
      "\n",
      "non-zero probability: 0.3\n",
      "matrix:\n",
      "[[1.74 0.   0.   0.   0.  ]\n",
      " [0.   0.48 0.47 0.   0.  ]\n",
      " [0.   0.   1.79 0.3  0.  ]\n",
      " [0.   0.   0.51 0.04 0.92]\n",
      " [0.   0.64 0.87 0.   0.77]]\n",
      "\n",
      "||Ax - b||: 267.61453516874695\n",
      "||x - y||: 2184.2864129647774\n",
      "\n",
      "non-zero probability: 0.5\n",
      "matrix:\n",
      "[[1.74 0.87 0.18 0.   0.93]\n",
      " [0.   0.48 0.   0.   0.  ]\n",
      " [0.37 0.53 1.79 0.   0.  ]\n",
      " [0.   0.   0.   0.07 0.  ]\n",
      " [0.56 0.   0.   0.91 0.77]]\n",
      "\n",
      "||Ax - b||: 0.0007878576685226291\n",
      "||x - y||: 0.0020848264244934696\n",
      "\n",
      "non-zero probability: 1\n",
      "matrix:\n",
      "[[1.74 0.87 0.18 0.85 0.93]\n",
      " [0.28 0.96 0.47 0.79 0.99]\n",
      " [0.37 0.53 1.79 0.3  0.31]\n",
      " [0.8  0.06 0.51 0.07 0.92]\n",
      " [0.56 0.64 0.87 0.91 1.55]]\n",
      "\n",
      "||Ax - b||: 1696896836.196236\n",
      "||x - y||: 840104528.8687085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "A_orig = np.random.rand(n, n)\n",
    "b = np.random.rand(n)\n",
    "for nonzero_element_odds in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 1]:\n",
    "\n",
    "    # all diagonal elemenents and random non-diagonal ones proportional to the probability\n",
    "    A = A_orig * (np.random.rand(n, n) < nonzero_element_odds) + np.diag(np.diag(A_orig))\n",
    "    x = gauss_seidel_iteration(A, b, tol=0.001)\n",
    "    residual_1 = np.linalg.norm(A@x - b)\n",
    "\n",
    "    y = np.random.rand(n)\n",
    "    b_y = A @ y\n",
    "    x_y = gauss_seidel_iteration(A, b_y, tol=0.001)\n",
    "    residual_2 = np.linalg.norm(x_y - y)\n",
    "    \n",
    "    print(f'non-zero probability: {nonzero_element_odds}')\n",
    "    print(f'matrix:')\n",
    "    print(f'{np.round(A,2)}' + '\\n') \n",
    "    print(f'||Ax - b||: {residual_1}')\n",
    "    print(f'||x - y||: {residual_2}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method for scalar nonlinear equation $f(x)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is tested for the second order polynomial. Only parameter $b$ is going to be changed, which geometrically means shifting the root of the parabola along the x-axis but still ensures the root exists and the function is differentiable. Also as the polynomial is second order it has two roots, but the positive one will be found by giving a positive starting point $x_0 = 2$\n",
    "\n",
    "The calculated residuals are $\\vert f(x) \\vert$ and $\\vert x-y \\vert$ where $y$ is the real root. Displayed as well is the width coefficient to see wether and by how much the residuals increase by having worse initial guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parabola width coefficient: 0\n",
      "|f(x)|: 0.00019290123456450203\n",
      "|x - y|: 4.3133611320467224e-05\n",
      "\n",
      "parabola width coefficient: 1\n",
      "|f(x)|: 7.561436672798294e-05\n",
      "|x - y|: 1.6500348166692547e-05\n",
      "\n",
      "parabola width coefficient: 10\n",
      "|f(x)|: 5.906383858444997e-06\n",
      "|x - y|: 5.391765854900754e-07\n",
      "\n",
      "parabola width coefficient: 100\n",
      "|f(x)|: 1.7810819485930551e-10\n",
      "|x - y|: 1.7780776850884195e-12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_polynomial(b):\n",
    "    \n",
    "    def polynomial(x):\n",
    "        return x**2 + b*x - 5\n",
    "    \n",
    "    # quadratic formula\n",
    "    root = (-b + np.sqrt(b**2 - 4*1*(-5))) / (2*1)\n",
    "    \n",
    "    return polynomial, root\n",
    "\n",
    "for n in [0, 1, 10, 100]:\n",
    "    function, real_root = create_polynomial(n)\n",
    "    newton_root = newtons_method(function, x0 = 2)\n",
    "    residual1 = abs(function(newton_root))\n",
    "    residual2 = (abs(newton_root) - abs(real_root))\n",
    "    \n",
    "    print(f'parabola width coefficient: {n}')\n",
    "    print(f'|f(x)|: {residual1}')\n",
    "    print(f'|x - y|: {residual2}')\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method for vector nonlinear equation $f(x)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is tested with one exact system of equations:\n",
    "\n",
    "$$\n",
    "f(x,y) = \\begin{cases}\n",
    "x^2 -2x + 5 \\\\\n",
    "xy + 10\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then the Jacobian. For demonstration the Jacobian matrix was found analytically, but could just as well be calculated with the same finite difference scheme.\n",
    "\n",
    "$$\n",
    "J(x, y) = \\begin{pmatrix}\n",
    "        2x & \n",
    "        -2 \\\\[1ex] % <-- 1ex more space between rows of matrix\n",
    "        x & \n",
    "        y \\\\[1ex]\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And the system has one root, which was calculated with $Mathematica$:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x = -3.3202 \\\\\n",
    "y = 3.0119\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As there is only one case, the method will be tested by varying the initial guess.\n",
    "The same as before, the calculated residuals are $\\vert f(x) \\vert$ and $\\vert x-y \\vert$ where $y$ is the real root. Displayed as well are the initial guesses, to show how far they are from the real roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_init, y_init: (-5, 1)\n",
      "|f(x)|: [2.07580555e-05 1.17307753e-04]\n",
      "|x - y|: [1.08130578e-05 2.55225522e-05]\n",
      "\n",
      "x_init, y_init: (-5, 3)\n",
      "|f(x)|: [2.23881825e-05 6.93110206e-05]\n",
      "|x - y|: [2.29020824e-06 1.87980395e-05]\n",
      "\n",
      "x_init, y_init: (-5, 10)\n",
      "|f(x)|: [9.10303015e-07 1.08431627e-06]\n",
      "|x - y|: [1.84923394e-07 1.58831281e-07]\n",
      "\n",
      "x_init, y_init: (-5, 100)\n",
      "|f(x)|: [7.43337560e-08 8.55559605e-08]\n",
      "|x - y|: [1.48876769e-08 1.22631958e-08]\n",
      "\n",
      "x_init, y_init: (-3, 1)\n",
      "|f(x)|: [3.06202881e-06 4.84215977e-06]\n",
      "|x - y|: [7.07161392e-07 8.16903557e-07]\n",
      "\n",
      "x_init, y_init: (-3, 3)\n",
      "|f(x)|: [0.00017222 0.000155  ]\n",
      "|x - y|: [9.32676996e-06 5.51444922e-05]\n",
      "\n",
      "x_init, y_init: (-3, 10)\n",
      "|f(x)|: [2.21841611e-05 4.29450810e-05]\n",
      "|x - y|: [5.68361344e-06 7.77867260e-06]\n",
      "\n",
      "x_init, y_init: (-3, 100)\n",
      "|f(x)|: [7.40565413e-08 9.71988712e-08]\n",
      "|x - y|: [1.56844155e-08 1.50471360e-08]\n",
      "\n",
      "x_init, y_init: (1, 1)\n",
      "|f(x)|: [0.00010756 0.00014761]\n",
      "|x - y|: [2.32384907e-05 2.33764160e-05]\n",
      "\n",
      "x_init, y_init: (1, 3)\n",
      "|f(x)|: [3.7624338  6.24601757]\n",
      "|x - y|: [4.27699837 6.93535213]\n",
      "\n",
      "x_init, y_init: (1, 10)\n",
      "|f(x)|: [17.52044239 29.08569209]\n",
      "|x - y|: [8.75556826 0.4995232 ]\n",
      "\n",
      "x_init, y_init: (1, 100)\n",
      "|f(x)|: [5.058954   8.39837117]\n",
      "|x - y|: [6.31864127 3.54602009]\n",
      "\n",
      "x_init, y_init: (3, 1)\n",
      "|f(x)|: [4.1010667  6.80818216]\n",
      "|x - y|: [5.89313581 4.25240183]\n",
      "\n",
      "x_init, y_init: (3, 3)\n",
      "|f(x)|: [0.00027195 0.00027896]\n",
      "|x - y|: [5.20401608e-05 3.68109923e-05]\n",
      "\n",
      "x_init, y_init: (3, 10)\n",
      "|f(x)|: [3.33576928 5.53771166]\n",
      "|x - y|: [4.71959035 6.20060503]\n",
      "\n",
      "x_init, y_init: (3, 100)\n",
      "|f(x)|: [4.41029717 7.32153575]\n",
      "|x - y|: [6.04917791 3.9933563 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def jacobian(x):\n",
    "\n",
    "    jac = np.array([[2*x[0], -2],\n",
    "                    [x[1],x[0]]])\n",
    "\n",
    "    return jac\n",
    "\n",
    "def function(x):\n",
    "    f = np.zeros(2)\n",
    "    f[0] = x[0]**2 - 2*x[1] - 5\n",
    "    f[1] = x[0]*x[1] + 10\n",
    "\n",
    "    return f\n",
    "\n",
    "real_roots = np.array([-3.3202006469833175893, 3.011866168114220354])\n",
    "for x_init in [-5, -3, 1, 3]:\n",
    "    for y_init in [1, 3, 10, 100]:\n",
    "\n",
    "\n",
    "        newton_roots = newton_vector(function, np.array([x_init, y_init]))\n",
    "\n",
    "        residual1 = abs(function(newton_roots))\n",
    "        residual2 = abs(newton_roots - real_roots)\n",
    "        \n",
    "        print(f'x_init, y_init: {x_init, y_init}')\n",
    "        print(f'|f(x)|: {residual1}')\n",
    "        print(f'|x - y|: {residual2}')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4GLBv0zWr7m"
   },
   "source": [
    "# **Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bcsDSoRXHZe"
   },
   "source": [
    "For the Jacobian and Gauss-Seidel iterations testing procedure was identical. It could be seen that initially with completely diagonal or diagonally dominated matrices the residuals converged to approximately zero, but when increasing the amount of non-diagonal elements the results started to diverge, as expected. Moreover the divergence started with around 30% non-diagonal elements being non-zero, indicating the importance of criterion. Of course, experiments with various tolerances, matrix dimensions and element amplitudes could be tested, but the preliminary tests already proved the necessity for being diagonally dominant for solution to be found.\n",
    "\n",
    "The Newton's method for scalar functions converged to approximately zero in all cases, despite changing the width of the parabola. A more complex function could be used for further evaluation.\n",
    "For the Newton's method for systems, the initial guesses showed to be more important, where the tests close to the roots converged, but the furthest one(especially in the x direction) diverged.\n",
    "\n",
    "\n",
    "In all cases further checks would be necessary to ensure the method can be applicable at all, like checking wether the matrices are invertible, functions differentiable etc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "template-report-lab-X.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
