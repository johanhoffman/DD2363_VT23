{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 1: Iterative methods**\n",
        "**Gustav Grevsten**"
      ],
      "metadata": {
        "id": "H6nLAQMX9XiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Abstract**\n",
        "\n",
        "The purpose of this lab is to implement and test iterative algorithms for finding solutions to both linear and non-linear systems. We implement algorithms for Jacobi Iteration, Gauss-Seidel iteration, and Newtons method for finding zero-solutions for both scalar and vector valued non-linear functions."
      ],
      "metadata": {
        "id": "PGVUrzKO-En0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set up environment**"
      ],
      "metadata": {
        "id": "Gvji8R1e_T2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load neccessary modules.\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8R4D8Kv0_XZb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "Iterative methods can be used for solving problems where direct methods are either too costly on time/storage, or for solving non-linear problems for which it may be difficult or impossible to find an exact solution analytically. The key idea is that the iterative process will eventually converge to the sought out solution, thus allowing for an arbitrarily good approximation. For these methods, it is important to make sure that the starting guesses and equations used fulfill the necessary conditions for convergence."
      ],
      "metadata": {
        "id": "-AoSAHDd_gfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Method**\n",
        "\n",
        "The algorithms we will employ in this lab are Jacobi Iteration, Gauss-Seidel iteration, and Newtons method both for scalar and vector valued functions.\n",
        "\n",
        "Firstly, Jacobi iteration can be used for finding vector solution $x$ to the equation $Ax = b$, for a given matrix $A$ and vector $b$. The process starts by selecting a starting guess for our initial value $x^0$ (Generally $x^0 = [0,0 ... 0]^T$) and then iterating on the value using the equation\n",
        "\n",
        "$x^{k+1}_i = \\frac{1}{a_{ii}}(b_i - \\sum_{j=1}^{n}a_{ij}x_j^k).$\n",
        "\n",
        "Note that this process is only guaranteed to converge if the matrix $A$ is strictly or irreducibly diagonally dominant (See the course book, chapter 7, page 152). It must therefore hold that\n",
        "\n",
        "$|a_{ii}| \\geq \\sum_{i \\neq j} |a_{ij}|, \\forall i$\n",
        "\n",
        "and there exists at least one index $k$ such that\n",
        "\n",
        "$|a_{kk}| > \\sum_{i \\neq j} |a_{kj}|$\n",
        "\n",
        "It should be noted that the process may still converge for other matrices.\n",
        "\n",
        "For more information, see [this article](https://en.wikipedia.org/wiki/Jacobi_method) on Jacobi iteration.\n",
        "\n",
        "The Jacobi iteration algorithm is implemented as python code below:"
      ],
      "metadata": {
        "id": "cvgGrntOJxxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jacobi_iteration(A,b):\n",
        "  dim = len(b)\n",
        "  thresh = 10**-6\n",
        "  x = []\n",
        "  for i in range(dim):\n",
        "    x.append(0)\n",
        "  x = np.array(x)\n",
        "  while np.linalg.norm(np.matmul(A,x) - b) > thresh:\n",
        "    x_temp = []\n",
        "    for i in range(dim):\n",
        "        sum = 0\n",
        "        for j in range(dim):\n",
        "          if i != j:\n",
        "            sum += A[i,j]*x[j]\n",
        "        x_temp.append((1/A[i,i])*(b[i] - sum))\n",
        "    x = np.array(x_temp)\n",
        "  return x"
      ],
      "metadata": {
        "id": "kh91_PRAR16W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will present the Gauss–Seidel method. This method is very similar to Jacobi iteration, except we use the updated values of $x^{k+1}$ as they are consecutively calculated. This means that we can overwrite the vector $x$, and we do not have to store two vectors (one for $x^k$ and one for $x^{k+1}$) as we do in Jacobi iteration, which can be useful for very large problems where much data must otherwise be stored. Each step in the algorithm is defined by the equation\n",
        "\n",
        "$x^{k+1}_i = \\frac{1}{a_{ii}}(b_i - \\sum_{j=1}^{i-1}a_{ij}x_j^{k+1} - \\sum_{j=i}^{n}a_{ij}x_j^{k}).$\n",
        "\n",
        "This algorithm follows the same converge criteria of $A$ being strictly or irreducibly diagonally dominant.\n",
        "\n",
        "For more information, see [this article](https://en.wikipedia.org/wiki/Jacobi_method) on the Gauss–Seidel method.\n",
        "\n",
        "The Gauss–Seidel method algorithm is implemented as python code below:"
      ],
      "metadata": {
        "id": "eTt4oNxjWqBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gauss_seidel(A,b):\n",
        "  dim = len(b)\n",
        "  thresh = 10**-6\n",
        "  x = []\n",
        "  for i in range(dim):\n",
        "    x.append(0)\n",
        "  x = np.array(x)\n",
        "  while np.linalg.norm(np.matmul(A,x) - b) > thresh:\n",
        "    x_temp = []\n",
        "    for i in range(dim):\n",
        "        sum = 0\n",
        "        for j in range(i):\n",
        "            sum += A[i,j]*x_temp[j]\n",
        "        for j in range(i+1,dim):\n",
        "            sum += A[i,j]*x[j]\n",
        "        x_temp.append((1/A[i,i])*(b[i] - sum))\n",
        "    x = np.array(x_temp)\n",
        "  return x"
      ],
      "metadata": {
        "id": "UFJ1WGjCaQVY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we present Newton's method for finding roots $x_0$ of real-valued functions such that $f(x_0) = 0$, where the derivative $f'(x)$ can either be directly calculated or approximated. The algorithm is defined by the equation\n",
        "\n",
        "$x^{k+1} = x^k - \\frac{f(x^k)}{f'(x^k)}.$\n",
        "\n",
        "Newton's method will usually converge for sufficiently close initial guesses $x^0$.\n",
        "\n",
        "For more information, see [this article](https://en.wikipedia.org/wiki/Newton%27s_method) on Newton's method.\n",
        "\n",
        "The algorithm for Newtons method is concisely implemented as python code below:"
      ],
      "metadata": {
        "id": "jy8Yw2wXboNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def der(f, x):\n",
        "  dx = 10**-10\n",
        "  df = f(x+dx) - f(x)\n",
        "  return df/dx\n",
        "\n",
        "def newton(f, guess):\n",
        "  x = guess\n",
        "  thresh = 10**-3\n",
        "  while np.absolute(f(x)) > thresh:\n",
        "    x -= f(x)/der(f, x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "tNlsZ8eKcm29"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, Newtons method can be extended to real vector valued functions $F(\\overline{x}) = \\overline{y}$. For this method, we use the Jacobian matrix in order to calculate each step, giving us the equation\n",
        "\n",
        "$\\overline{x}^{k+1} = \\overline{x}^k - J_F(\\overline{x}^k)^{-1}F(\\overline{x}^k).$\n",
        "\n",
        "The algorithm is implemented as python code below:"
      ],
      "metadata": {
        "id": "aqJNDyzlfB-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_der(f, x, index):\n",
        "  dx = 10**-10\n",
        "  dv = np.copy(x)\n",
        "  dv[index] += dx\n",
        "  df = f(dv) - f(x)\n",
        "  return df/dx\n",
        "\n",
        "def jacobian(f,x):\n",
        "  dim = len(x)\n",
        "  J = np.zeros((dim, dim))\n",
        "  for i in range(dim):\n",
        "    for j in range(dim):\n",
        "      J[i,j] = vec_der(f, x, j)[i]\n",
        "  return J\n",
        "\n",
        "def vec_newton(f, guess):\n",
        "  x = guess\n",
        "  thresh = 10**-3\n",
        "  while np.linalg.norm(f(x)) > thresh:\n",
        "    Df = jacobian(f,x)\n",
        "    dx = np.linalg.solve(Df, -f(x))\n",
        "    x += dx\n",
        "  return x"
      ],
      "metadata": {
        "id": "5QXtNf2yeXTN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**\n",
        "\n",
        "We test all of the algorithms presented in the Methods section below. We start by creating an arbitrary vector $b$ and a diagonally dominant matrix $A$:"
      ],
      "metadata": {
        "id": "ov4F2AsbhMsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CX7LW-UN_RSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3777c552-4c35-48d4-92ac-835022c92c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacobi iteration\n",
            "x = [-1.05033707 17.56314608 32.79235957]\n",
            "Ax = [123.00000021 456.00000038 789.00000054]\n",
            "b = [123 456 789]\n",
            "\n",
            "Gauss-Seidel\n",
            "x = [-1.05033703 17.56314608 32.79235953]\n",
            "Ax = [123.00000045 456.0000002  789.        ]\n",
            "b = [123 456 789]\n"
          ]
        }
      ],
      "source": [
        "b = np.array(\n",
        "    [123,\n",
        "     456,\n",
        "     789])\n",
        "\n",
        "A = np.array([\n",
        "    [10,2,3],\n",
        "    [4,15,6],\n",
        "    [7,8,20]\n",
        "    ])\n",
        "\n",
        "x = jacobi_iteration(A,b)\n",
        "\n",
        "print(\"Jacobi iteration\")\n",
        "print(\"x = \" + str(x))\n",
        "print(\"Ax = \" + str(np.matmul(A,x)))\n",
        "print(\"b = \" + str(b))\n",
        "print(\"\")\n",
        "\n",
        "x = gauss_seidel(A,b)\n",
        "\n",
        "print(\"Gauss-Seidel\")\n",
        "print(\"x = \" + str(x))\n",
        "print(\"Ax = \" + str(np.matmul(A,x)))\n",
        "print(\"b = \" + str(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, both methods converge to the same values, and give highly accurate results. Next, we test Newton's method applied to the function $cos(x)$"
      ],
      "metadata": {
        "id": "sSkWpPR-iWf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return np.cos(x)\n",
        "\n",
        "sol = newton(f, 1)\n",
        "\n",
        "print(\"x_0 = \" + str(sol))\n",
        "print(\"f(x_0) = \" + str(f(sol)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhrJu9JiiVxL",
        "outputId": "671aaca3-b897-48ff-8c3f-8e2aea78a60d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_0 = 1.5706752856746578\n",
            "f(x_0) = 0.00012104111994330725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the method closely approximated the root $x = \\frac{\\pi}{2}$. Finally, we test Newton's method for vector valued function $F((x,y,z)^T) = (cos(y),zsin(x)-z,z^2-\\frac{\\pi}{2})^T$."
      ],
      "metadata": {
        "id": "gpZphzUnkuDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_f(x):\n",
        "  return np.array([np.cos(x[1]), x[2]*np.sin(x[0]) - x[2], x[2]**2 - np.pi**2])\n",
        "\n",
        "test = vec_newton(vec_f, np.array([1.57, 1, 0.1]))\n",
        "\n",
        "print(\"solution x = \" + str(test))\n",
        "print(\"F(x) = \" + str(vec_f(test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq_SdtQZTrJZ",
        "outputId": "231ae1d9-e5c1-4033-f698-2a6a971a3185"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solution x = [1.58217469 1.57079633 3.14159318]\n",
            "F(x) = [ 6.12323400e-17 -2.03364333e-04  3.28464474e-06]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which closely approximates the solution $(x,y,z)^T = (\\frac{\\pi}{2},\\frac{\\pi}{2},\\pi)^T$"
      ],
      "metadata": {
        "id": "GJYJBHnVnohI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discussion**\n",
        "\n",
        "As expected, we were able to find good approximate solutions for the various problems using the various iterative methods. Ultimately, this shows that they can be fast and powerful tools for solving hard problems."
      ],
      "metadata": {
        "id": "km7HBucKoYXC"
      }
    }
  ]
}